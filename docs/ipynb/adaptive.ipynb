{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5822ec2",
   "metadata": {},
   "source": [
    "# Adaptive time step methods\n",
    "\n",
    "In practical computations, one seeks to achieve a desired accuracy with the minimum \n",
    "computational effort. For a given method, this requires finding the largest possible \n",
    "value of the time step $\\Delta t$. In the previous chapters we always kept the\n",
    "step size constant through the solution interval, but this is rarely the most \n",
    "efficient approach, since the error depends on the characteristics of the \n",
    "solution in addition to the step size. In regions where the solution is smooth, \n",
    "large time steps can be used without introducing significant error, and in regions where the \n",
    "solution has rapid variations, a smaller time step must be employed. In this chapter we\n",
    "will extend the Runge-Kutta methods from the previous chapters, to methods that select\n",
    "the time step automatically to control the error in the solution. \n",
    "\n",
    "\n",
    "# A motivating example\n",
    "Many ODE models of dynamic systems have solutions that vary rapidly in some intervals and are\n",
    "nearly constant in others. As a motivating example, we may consider a particular class of ODE \n",
    "models that which describe the so-called *action potential* of excitable cells. These models, \n",
    "first introduced by\n",
    "Hodgkin and Huxley [[Hodgkin]](#Hodgkin), are important tools for studying the electrophysiology \n",
    "of cells such as neurons and different types of muscle cells. The main variable of interest\n",
    "is usually the transmembrane potential, which is the difference in electrical potential between\n",
    "the internals of a cell and its surroundings. When an excitable cell such as a neuron or\n",
    "a muscle cell is stimulated electrically, it triggers a cascade of processes in the\n",
    "cell membrane, leading to various ion channels opening and closing, and the membrane\n",
    "potential going from its resting negative state to approximately zero or slightly positive, \n",
    "before returning to the resting value. This process of *depolarization* followed by \n",
    "*repolarization* is called the action potential, and is illustrated in [Figure](#fig:hodgkinhuxley).\n",
    "See, for instance, [[KeenerSneyd]](#KeenerSneyd), for a comprehensive overview of the Hodgkin-Huxley model\n",
    "and action potential models in general.\n",
    "The potential utility of adaptive time step methods is obvious from [Figure](#fig:hodgkinhuxley). \n",
    "The solution changes rapidly during the action potential, but is approximately constant for long\n",
    "time intervals when the cell is at rest. Such behavior can be observed in many \n",
    "types of ODE models, and  motivates methods that can adjust the time step to the properties of the\n",
    "solution. Commonly referred to as adaptive methods or methods with\n",
    "automatic time step control, these techniques are important parts of\n",
    "all modern ODE software.\n",
    "\n",
    "<!-- dom:FIGURE: [./figs_ch4/hodgkinhuxley_AP.png, width=600 frac=1] Solution of the Hodgkin-Huxley model. The left panel shows a single action potential, while the right panel shows the result of stimulating the cell multiple times with a fixed period. <div id=\"fig:hodgkinhuxley\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:hodgkinhuxley\"></div>\n",
    "\n",
    "<p>Solution of the Hodgkin-Huxley model. The left panel shows a single action potential, while the right panel shows the result of stimulating the cell multiple times with a fixed period.</p>\n",
    "<img src=\"./figs_ch4/hodgkinhuxley_AP.png\" width=600>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "There are many possible approaches for selecting the time step automatically. One intuitive approach is to\n",
    "base the time step estimate on the dynamics of the solution, and select a small time step whenever rapid \n",
    "variations occur. This approach is commonly applied in adaptive solvers for partial differential equations (PDEs), \n",
    "where both the time step and space step can be chosen adaptively. It has also been successfully applied\n",
    "in specialized solvers for the action potential models mentioned above, see, e.g., [[RushLarsen]](#RushLarsen),\n",
    "where the time step is simply selected based on the variations of the transmembrane voltage. However, \n",
    "this method may not be universally applicable and the criteria for choosing the time step must be \n",
    "carefully selected based on the characteristics of the problem at hand. \n",
    "\n",
    "# Choosing the time step based on the local error\n",
    "The goal of an adaptive time stepping method is to control the error in the solution, and it is natural to \n",
    "base the step selection on some form of error estimate. In Chapter 1 we\n",
    "computed the global error in the solution, which may of course also be useful for selecting the\n",
    "time step. If the global error is too large, we simply redo the computation with a smaller time step. \n",
    "However, the goal for the adaptive time step methods is to select the time step dynamically \n",
    "as the solution progresses, to ensure that the final solution satisfies a given error tolerance. \n",
    "This goal requires a different approach, which is based on estimates of the local error for each \n",
    "step rather than the global error. \n",
    "\n",
    "Assuming that we are able to compute an estimate for the local error for a given step, $e_n$,  \n",
    "the goal is to choose the time step $\\Delta t_n$ so that the inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3922352",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"error0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e_n < tol\n",
    "\\label{error0} \\tag{1}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2616b",
   "metadata": {},
   "source": [
    "is satisfied for all steps. There are two essential parts to the process of choosing\n",
    "$\\Delta t_n$ so that ([1](#error0)) is always satisfied. The first is that we always\n",
    "check the inequality after a step is performed. If it is satisfied we accept the step and\n",
    "proceed with step $n+1$ as normal, and if it is not satisfied we reject the step and \n",
    "try again with a smaller $\\Delta t_n$. The second part of the procedure is to choose the\n",
    "next time step, that is, either $\\Delta t_{n+1}$ if the current step was accepted, or a new\n",
    "guess for $\\Delta t_n$ if it was rejected. We shall see that the same formula, derived\n",
    "from what we know about the local error, can be applied in both cases.\n",
    "\n",
    "We first assume, for simplicity of notation, that step $n$ was accepted with a time step\n",
    "$\\Delta t$ and a local error estimate $e_n < tol$. Our aim is now to choose $\\Delta t_{n+1}$\n",
    "so that ([1](#error0)) is satisfied as sharply as possible to avoid wasting\n",
    "computations, so we want to choose $\\Delta t_{n+1}$ so that $e_{n+1} \\lessapprox tol$. \n",
    "For a method of order $p$, the local error is of order $p+1$, so we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f28086",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"error_0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e_n \\approx C (\\Delta t_n)^{p+1}  \\label{error_0} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7db1e4",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"error_1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "e_{n+1} \\approx C (\\Delta t_{n+1})^{p+1}  \\label{error_1} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b508c",
   "metadata": {},
   "source": [
    "where we have assumed that the error constant $C$ is constant from one step to the next. \n",
    "Eq. ([2](#error_0)) gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe67f08",
   "metadata": {},
   "source": [
    "$$\n",
    "C =  \\frac{e_n}{(\\Delta t_n)^{p+1}} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d893ce",
   "metadata": {},
   "source": [
    "and inserting into ([3](#error_1)) gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb2d0a",
   "metadata": {},
   "source": [
    "$$\n",
    "e_{n+1} \\approx \\frac{e_n}{(\\Delta t_n)^{p+1}} (\\Delta t_{n+1})^{p+1}  .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8239cff7",
   "metadata": {},
   "source": [
    "We want to have $e_{n+1} \\approx tol$, so we set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debf8e7",
   "metadata": {},
   "source": [
    "$$\n",
    "tol = e_{n+1} = \\frac{e_n}{\\Delta t_n^{p+1}} \\Delta t_{n+1}^{p+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1431d1",
   "metadata": {},
   "source": [
    "and rearrange to get the standard formula for time step selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e36ace",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta t_{n+1} = \\left( \\frac{tol}{e_n} \\Delta t_n^{p+1}  \\right)^{1/(p+1)} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd431b",
   "metadata": {},
   "source": [
    "We see that if $e_n \\ll tol$ this formula will select a larger step size for the next step,\n",
    "while if $e_n \\approx tol$ we get $\\Delta t_{n+1} \\approx \\Delta t_n$. In practice,\n",
    "the formula is usually modified with a safety factor, i.e., we set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038106f",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"step_size0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta t_{n+1} = \\eta \\left( \\frac{tol}{e_n} \\Delta t_n^{p+1}  \\right)^{-(p+1)} .\n",
    "\\label{step_size0} \\tag{4}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3001cfd",
   "metadata": {},
   "source": [
    "for some $\\eta < 1$. The exact same formula can be used to choose a new step size $\\Delta t_n$ \n",
    "if the step was rejected, i.e., if $e_n > tol$.  \n",
    "\n",
    "While ([4](#step_size0)) gives a simple formula for the step size, and we shall see later that it\n",
    "works well for our example problems, more sophisticated methods have been derived. The \n",
    "problem of choosing the time step to control the error is an optimal control problem, and\n",
    "successful methods have been derived based on control theory, in order to control the error while\n",
    "avoiding rapid variations in the step size. See, for instance, [[ODEII]](#ODEII) for details and examples \n",
    "of such methods. \n",
    "\n",
    "# Estimating the local error\n",
    "  The inequality ([1](#error0)) and formula ([4](#step_size0)) gives the necessary tools to select the time\n",
    "step based on the local error $e_n$. The remaining task is to come up with a method to estimate this\n",
    "error. It is, of course, not possible to compute it directly since the analytical solution is\n",
    "not available, but it can instead be estimated based on two numerical solution of different accuracy.\n",
    "We described in Chapter 1 how the global error could be estimated in this way, and\n",
    "the idea is exactly the same for the local error. We simply advance the solution\n",
    "from $t_{n-1}$ to $t_{n}$ twice, using two methods of different accuracy, giving us our regular\n",
    "solution $u_{n}$ and a more accurate one $\\hat{u}_{n}$. The difference $|\\hat{u}_{n}-u_{n}|$ can \n",
    "then be used to estimate the local error for the solution $u_{n}$. The more accurate \n",
    "solution $\\hat{u}_{n}$ can be computed in two ways; either by taking several \"internal\" time steps to advance\n",
    "from $t_n$ to $t_{n}$, or by using a method with higher order of accuracy. The first approach is\n",
    "the foundation for a technique referred to as *step doubling*, where the solution $\\hat{u}_{n+1}$\n",
    "is computed with the same method used for $u_{n+1}$, but using two steps of length $\\Delta t/2$\n",
    "instead of one step $\\Delta t$. This obviously makes $\\hat{u}_{n+1}$ more accurate than $u_{n+1}$,\n",
    "but the difference is not very large, so the difference $|\\hat{u}_{n+1}-u_{n+1}|$ cannot be used directly\n",
    "as an error estimate. However, an error estimate may be derived by combining this difference with\n",
    "the known order of the method, see [[AscherPetzold]](#AscherPetzold) for details. The step doubling method is\n",
    "completely general and can be used to provide a local error estimate for all ODE solvers. However, \n",
    "it is also computationally expensive, and most modern ODE software are based on other techniques. \n",
    "The second approach for computing $\\hat{u}_n$, to use a method with higher order of accuracy, \n",
    "turns out to be particularly attractive for RK methods. We shall see in the next section that it\n",
    "is possible to construct so-called *embedded methods*, which provides an error estimate with \n",
    "very little additional computation. \n",
    "\n",
    "\n",
    "## Error estimates from embedded methods\n",
    " For a numerical method of order $p$, a solution computed with a method of higher order\n",
    "for instance $p+1$, can be used to estimate the local error. Since $\\Delta t $ is small, \n",
    "we have $\\Delta t^{p+1} \\ll \\Delta t^p$, and the error can be estimated directly as \n",
    "$e_{n} = |u_n - \\hat{u}_n|$. It would be very expensive to compute these two solutions using\n",
    "two entirely different methods, but the error estimate can often be obtained more \n",
    "efficiently by embedded methods. An embedded method is a variation of a given \n",
    "RK method that uses the same stage computations as the original method, but achieves a \n",
    "different order of accuracy. Since most of the computational work in RK methods occurs \n",
    "in the stage computations, error estimates based on embedded methods are relatively cheap to evaluate.\n",
    "\n",
    "For the general RK method defined by \n",
    "([genrk0](#genrk0))-([genrk1](#genrk1)), an embedded method can be introduced by defining a separate \n",
    "set of weights $\\hat{b}_i$, which advance the solution using the same $k_i$ as the main method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e420e",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"ch4:rkpair1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "k_i = f(t_n+c_i\\Delta t,y_n+\\Delta t\n",
    "\\sum_{j=1}^s a_{ij}k_j)  \\mbox{ for } i = 1,\\ldots ,s \\label{ch4:rkpair1} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988d95e",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"ch4:rkpair2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "u_{n+1} = u_n + \\Delta t \\sum_{i=1}^s b_i k_i, \\label{ch4:rkpair2} \\tag{6} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bcf28",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"ch4:rkpair3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\hat{u}_{n+1}= u_n + \\Delta t \\sum_{i=1}^s \\hat{b}_i k_i . \\label{ch4:rkpair3} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c25e57",
   "metadata": {},
   "source": [
    "Although the main idea is to reuse the same stage computations to compute both \n",
    "$\\hat{u}_{n+1}$ and $u_{n+1}$, it is not uncommon to introduce one additional \n",
    "stage in the method to obtain the error estimate. An RK method\n",
    "with an embedded method for error estimation is often referred to as an RK\n",
    "pair of order $n(m)$, where $n$ is the order of the main method\n",
    "and $m$ the order of the method used for error estimation. \n",
    "Butcher tableaus for RK pairs are written exactly as before, but with one \n",
    "extra line for the additional coefficients $\\hat{b}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec6417",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c|ccc}\n",
    "c_i & a_{11} & \\cdots & a_{1s}\\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "c_s & a_{s1} & \\cdots & a_{ss} \\\\ \\hline\n",
    " & b_1 & \\cdots & b_s \\\\\n",
    " & \\hat{b}_1 & \\cdots & \\hat{b}_s   \n",
    "\\end{array} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042dbe3",
   "metadata": {},
   "source": [
    "As an example we may consider the simplest possible embedded RK pair, which is obtained by \n",
    "combining Heun's method with the forward Euler method. The method is defined by the Butcher Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c4590",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"heun_euler0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{array}{c|cc}\n",
    "0 & 0 & \\\\ \n",
    "1 & 1 & \\\\ \\hline\n",
    "& 1 & 0 \\\\ \\hline\n",
    "& 1/2 & 1/2 \n",
    "\\end{array} ,\n",
    "\\label{heun_euler0} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70842a27",
   "metadata": {},
   "source": [
    "which translates to the following formulas for advancing the two solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afcfa7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "k_1 &= f(t_n,u_n),\\\\\n",
    "k_2 &= f(t_n+\\Delta t,u_n+\\Delta t k_1), \\\\\n",
    "u_{n+1} &= u_n + \\Delta t k_1, \\\\\n",
    "\\hat{u}_{n+1} &= u_n + \\Delta t/2 (k_1+k_2) . \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92deb080",
   "metadata": {},
   "source": [
    "In the next section we will see how this method pair can be implemented \n",
    "as an extension of the `ODESolver` hierarchy introduced earlier, before\n",
    "we introduce more advanced embedded RK methods in the section [More advanced embedded RK methods](#sec:embedded2).\n",
    "\n",
    "# Implementing an adaptive solver\n",
    "In the previous chapters we have been able to reuse significant parts of the original `ODESolver` base class for all\n",
    "the RK method. The subclasses for the explicit RK methods needed to reimplement the `advance` method, \n",
    "while the implicit methods required a few additional methods, and it was also convenient to redesign the classes to \n",
    "define the method coefficients as attributes in the constructor. However, all the subclasses could reuse the `solve` \n",
    "method which contained the main solver loop. A quick inspection of this method reveals that the assumption of a fixed number\n",
    "of time steps is quite fundamental to the implementation, since it is based on a for-loop and NumPy arrays with fixed size. \n",
    "With an adaptive step size the number of steps is obviously not fixed, and we therefore need to change the `solve` method significantly. \n",
    "In fact, the only part of the original `ODESolver` class that can be reused directly is the `set_initial_condition`, which is \n",
    "obviously a very moderate benefit. However, it can still make sense to implement the adaptive methods as subclasses of `ODESolver`, to \n",
    "benefit from this tiny code reuse and to highlight that an adaptive solver is in fact a special case of a general ODE solver. \n",
    "Since most of the new functionality needed by adaptive solvers is generic to all adaptive solvers, it makes sense to implement them\n",
    "in a general base class. In summary, the following changes and additions are needed:\n",
    "\n",
    "* A complete rewrite of the `solve` method, to replace the for-loop and NumPy arrays with lists and a while loop. Lists\n",
    "  are usually not preferred for computational tasks, but for adaptive time step methods their flexible size makes them\n",
    "  attractive. It is also natural to add more parameters to the `solve` function, to let the user specify the tolerance and a \n",
    "  maximum and minimum step size.\n",
    "\n",
    "* The `advance` method needs to be updated to return both the updated solution and the error estimate.\n",
    "\n",
    "* The step selection formula in ([4](#step_size0)) must be implemented in a separate method.\n",
    "\n",
    "* Adaptive methods usually include a number of additional parameters, such as the safety factor $\\eta$ and the order $p$ used \n",
    "  in ([4](#step_size0)). These parameters are conveniently defined as attributes in the constructor.\n",
    "\n",
    "An implementation of the adaptive base class may look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a344a58b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ODESolver import *\n",
    "from math import isnan, isinf \n",
    "\n",
    "class AdaptiveODESolver(ODESolver):\n",
    "    def __init__(self, f, eta=0.9):\n",
    "        super().__init__(f)\n",
    "        self.eta = eta\n",
    "  \n",
    "    def new_step_size(self,dt,loc_error):\n",
    "        eta = self.eta\n",
    "        tol = self.tol\n",
    "        p = self.order\n",
    "        if isnan(loc_error) or isinf(loc_error):\n",
    "            return self.min_dt\n",
    "\n",
    "        new_dt = eta * (tol/loc_error)**(1/(p+1)) * dt\n",
    "        new_dt = max(new_dt,self.min_dt)\n",
    "        return min(new_dt,self.max_dt)\n",
    "        \n",
    "\n",
    "    def solve(self,t_span,tol=1e-3,max_dt=np.inf,min_dt=1e-5):\n",
    "        \"\"\"Compute solution for t_span[0] <= t <= t_span[1]\"\"\"\n",
    "        t0,T = t_span\n",
    "        self.tol = tol\n",
    "        self.min_dt = min_dt\n",
    "        self.max_dt = max_dt\n",
    "        self.t = [t0] \n",
    "        \n",
    "        if self.neq == 1:\n",
    "            self.u = [np.asarray(self.u0).reshape(1)]\n",
    "        else:\n",
    "            self.u = [self.u0]\n",
    "        \n",
    "        self.n = 0\n",
    "        self.dt = 0.1/np.linalg.norm(self.f(t0,self.u0))\n",
    "        \n",
    "        loc_t = t0\n",
    "        while loc_t < T:\n",
    "            u_new, loc_error = self.advance()\n",
    "            if loc_error < tol or self.dt < self.min_dt:\n",
    "                loc_t += self.dt\n",
    "                self.t.append(loc_t)\n",
    "                self.u.append(u_new)\n",
    "                self.dt = self.new_step_size(self.dt,loc_error)\n",
    "                self.dt = min(self.dt, T-loc_t, max_dt)\n",
    "                self.n += 1\n",
    "            else:\n",
    "                self.dt = self.new_step_size(self.dt,loc_error)\n",
    "        return np.array(self.t), np.array(self.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb9258",
   "metadata": {},
   "source": [
    "The constructor should be self-explanatory, but the other two methods deserve a few comments. The `step_size` method \n",
    "is essentially a Python implementation of ([4](#step_size0)), with tests to ensure that the selected step size \n",
    "is within the user defined range. We have also added a check which ensures\n",
    "that if the computed error is infinity or not a number (`inf` or `nan`) the new step size is automatically \n",
    "set to the minimum step size. This test is important for the robustness of the solver, since explicit methods\n",
    "will often diverge and return `inf` or `nan` values if applied to very stiff problems. Checking for these values\n",
    "and setting a low step size if they occur will therefore reduce the risk of complete solver failure. The small step\n",
    "size will still make the computation inefficient, but this is far better than unexpected failure. \n",
    "The `solve` method has also been substantially changed from the `ODESolver` version. \n",
    "First, the parameter list has been expanded to include the tolerance as\n",
    "well as the maximum and minimum time step. These are all stored as attributes and used in the main loop. The truly significant\n",
    "changes start with the initialization of the attributes `self.t` and `self.u`, which are now lists of \n",
    "length one rather than fixed size NumPy arrays. Notice also the somewhat cumbersome initialization \n",
    "of `self.u`, which includes an if-test that checks if we solve a scalar ODE or a system. This initialization \n",
    "ensures that for scalar equations, `self.u[0]` is a one-dimensional array of length one, rather than a \n",
    "zero-dimensional array. The actual contents of these two data structures is the same, i.e., a single number,\n",
    "but they are treated differently by some NumPy tools and it is useful to make sure that `self.u[0],self.u[1]`, \n",
    "and so forth all have the same dimensions. The first step size is then calculated using a simplified \n",
    "version of the algorithm outlined in [[ODEI]](#ODEI). The for-loop has been replaced by a while-loop, since the number of steps is initially unknown. The call to the\n",
    "`advance`-method gives the updated solution and the estimated local error, and we proceed to check if the local error is lower \n",
    "than the tolerance. If it is, the new time point and solution are appended to the corresponding lists, and the next time step \n",
    "is chosen based on the current one and the local error. The min and max operations are included to ensure that the time\n",
    "step is within the selected bounds, and that the simulation actually ends at the final time `T`. If the constraint \n",
    "`loc_error < tol` is not satisfied, we simply compute a new time step and try again, without updating the lists for the time\n",
    "and the solution. \n",
    "\n",
    "While the `solve` loop in the `AdaptiveODESolver` class is obviously a lot more complex than the earlier versions, it should be\n",
    "noted that it is still a very simple version of an adaptive solver. The aim here is to present the fundamental ideas \n",
    "and promote the general understanding of how these solvers are implemented, and we therefore only include the most essential parts. \n",
    "Important limitations and simplifications include the following:\n",
    "* As noted above, the step size selection in ([4](#step_size0)), implemented in `step_size`, could be replaced with more sophisticated\n",
    "  versions. See, for instance, [[ODEII;@deuflhard2012scientific]](#ODEII;@deuflhard2012scientific) for details. \n",
    "\n",
    "* The formula for selecting the initial step is very simple, and is mainly suitable for avoiding extremely bad choices for the\n",
    "  initial step size. More sophisticated algorithms have been derived, and we refer to, for instance, [[ODEI;@ODEII]](#ODEI;@ODEII) for details.\n",
    "\n",
    "* The first `if`-test inside the solver loop is not the most robust, since it will accept the stem and move forward if the minimum\n",
    "  step size is reached, even if the error is too large. A robust solver should in this case give the user a warning that the requested\n",
    "  tolerance cannot be reached.\n",
    "\n",
    "In spite of these and other limitations, the adaptive solver class works as intended, and captures the essential behavior of\n",
    "adaptive ODE solvers. \n",
    "\n",
    "With the `AdaptiveODESolver` base class at hand, subclasses for specific solvers can be implemented by writing specific \n",
    "versions of the `advance` method and the constructor, since the order of the method is used in the time step selection and therefore\n",
    "needs to be defined as an attribute. For the Euler-Heun method pair listed above, a suitable implementation may look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334d8ab8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EulerHeun(AdaptiveODESolver):  \n",
    "    def __init__(self, f, eta=0.9):\n",
    "        super().__init__(f,eta)\n",
    "        self.order = 1\n",
    "\n",
    "    def advance(self):\n",
    "        u, f, t = self.u, self.f, self.t\n",
    "        dt = self.dt \n",
    "        k1 = f(t[-1], u[-1])\n",
    "        k2 = f(t[-1] + dt, u[-1] + dt*k1)\n",
    "        high = dt/2*(k1+k2)\n",
    "        low = dt*k1\n",
    "        \n",
    "        unew = u[-1] + low\n",
    "        error = np.linalg.norm(high-low)\n",
    "        return unew, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442bbd",
   "metadata": {},
   "source": [
    "After computing the two stage derivatives `k1` and `k2`, the method computes the high and low order solution updates. The low order is\n",
    "used to advance the solution, while the difference between the two provides the error estimate. The method returns the updated solution\n",
    "and the error, as needed by the `solve` method implemented in the base class above. \n",
    "\n",
    "Since we have two methods with different accuracy, we may ask whether it would be better to advance the solution using the \n",
    "most accurate rather than the least accurate method. This choice will, of course, give a reduced local error, but the obvious downside\n",
    "is that we would no longer have a proper error estimate. We can use the more accurate solution to estimate the error of the\n",
    "less accurate, but not the other way around. However, the approach, known as *local extrapolation* [[ODEI]](#ODEI) is still \n",
    "used by many popular RK pairs, as we shall see in examples below. Even if the error estimate is then no longer a proper error \n",
    "estimate for the method used to integrate the solution, it still works well as a tool for selecting the time step. In the implementation\n",
    "above it is very easy to play around with this choice, by replacing `low` with `high` in the assignment of `unew`, and check \n",
    "the effect on the error and the number of time steps. \n",
    "\n",
    "\n",
    "# More advanced embedded RK methods\n",
    "<div id=\"sec:embedded2\"></div>\n",
    " There are numerous examples of explicit RK pairs of higher order than the 1(2) pair \n",
    "defined by ([8](#heun_euler0)). We will not provide an exhaustive list here, but mention\n",
    "two particularly popular methods, which have been implemented in various software packages.\n",
    "The first is a method by Fehlberg, often referred to as the Fehlberg 4(5) or simply the RKF45 \n",
    "method [[fehlberg1970]](#fehlberg1970). The Butcher tableau is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec5ea1",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"rkf45\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{array}{r|cccccc}\n",
    "  0&&&&&&\\\\\n",
    "  \\frac{1}{4}&\\frac{1}{4}&&&&\\\\\n",
    "  \\frac{3}{8}&\\frac{3}{32}&\\frac{9}{32}&&&\\\\\n",
    "  \\frac{12}{13}&\\frac{1932}{2197}&-\\frac{7200}{2197}&\\frac{7296}{2197}&&\\\\\n",
    "  1&\\frac{439}{216}&-8&\\frac{3680}{513}&-\\frac{845}{4104}&&\\\\\n",
    "  \\frac{1}{2}&-\\frac{8}{27}&2&-\\frac{3544}{2565}&\\frac{1859}{4104}&-\\frac{11}{40}&\\\\ \\hline \n",
    "  &\\frac{25}{216}&0&\\frac{1408}{2565}&\\frac{2197}{4104}&-\\frac{1}{5}&0 \\\\\n",
    "  &\\frac{16}{135}&0&\\frac{6656}{12825}&\\frac{28561}{56430}&-\\frac{9}{50}&\\frac{2}{55} \n",
    "\\end{array}\\hspace{0.5cm}, \n",
    "\\label{rkf45} \\tag{9}\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91ee2f",
   "metadata": {},
   "source": [
    "Here, the first line of $b$-coefficients ($b_i$) yields a fourth order method, while the \n",
    "bottom line ($\\hat{b}_i$) gives a method of order five. \n",
    "The implementation of the RKF45 method is similar to the Euler-Heun pair, although the number\n",
    "of stages and coefficients makes the `advance` method considerably more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c75a1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RKF45(AdaptiveODESolver):\n",
    "    def __init__(self, f, eta=0.9):\n",
    "        super().__init__(f, eta)\n",
    "        self.order = 4\n",
    "\n",
    "    def advance(self):\n",
    "        u, f, t = self.u, self.f, self.t\n",
    "        dt = self.dt \n",
    "        c2 = 1/4; a21 = 1/4; \n",
    "        c3 = 3/8; a31 = 3/32; a32 = 9/32\n",
    "        c4 = 12/13; a41 = 1932/2197; a42 = -7200/2197; a43 = 7296/2197\n",
    "        c5 = 1; a51 = 439/216; a52 = -8; a53 = 3680/513; a54 = -845/4104\n",
    "        c6 = 1/2; a61 = -8/27; a62 = 2; a63 = -3544/2565; \n",
    "        a64 = 1859/4104; a65 = -11/40\n",
    "        b1 = 25/216; b2 = 0; b3 = 1408/2565; b4 = 2197/4104; \n",
    "        b5 = -1/5; b6 = 0\n",
    "        bh1 = 16/135; bh2 = 0; bh3 = 6656/12825; bh4 = 28561/56430; \n",
    "        bh5 = -9/50; bh6 = 2/55\n",
    "\n",
    "        k1 = f(t[-1], u[-1])\n",
    "        k2 = f(t[-1] + c2*dt, u[-1] + dt*(a21*k1))\n",
    "        k3 = f(t[-1] + c3*dt, u[-1] + dt*(a31*k1+a32*k2))\n",
    "        k4 = f(t[-1] + c4*dt, u[-1] + dt*(a41*k1+a42*k2+a43*k3))\n",
    "        k5 = f(t[-1] + c5*dt, u[-1] + dt*(a51*k1+a52*k2+a53*k3+a54*k4))\n",
    "        k6 = f(t[-1] + c6*dt, u[-1] + \n",
    "               dt*(a61*k1+a62*k2+a63*k3+a64*k4+a65*k5))\n",
    "\n",
    "        low  = dt*(b1*k1+b3*k3+b4*k4+b5*k5)\n",
    "        high = dt*(bh1*k1+bh3*k3+bh4*k4+bh5*k5+bh6*k6)\n",
    "        \n",
    "        unew = u[-1] + low\n",
    "        error = np.linalg.norm(high-low)\n",
    "\n",
    "        return unew, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25582794",
   "metadata": {},
   "source": [
    "The `advance` method could obviously be written more compactly, but we chose to keep the structure of\n",
    "the explicit RK methods introduced earlier. \n",
    "\n",
    "Another famous and widely used pair of ERK methods is the Dormand-Prince method [[dormand]](#dormand), which is a \n",
    "seven-stage method with the following coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1254506",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand\\arraystretch{1.2}\n",
    "\\begin{array}{c|ccccccc}\n",
    "0 & & & & & & &\\\\ %\\vspace{0.2cm}\n",
    "\\frac{1}{5} & \\frac{1}{5} & & & & & & \\\\ %\\vspace{0.2cm}\n",
    "\\frac{3}{10}& \\frac{3}{40} & \\frac{9}{40} & & & & & \\\\ %\\vspace{0.2cm}\n",
    "\\frac{4}{5} & \\frac{44}{45} & -\\frac{56}{15} & \\frac{32}{9} & & & &  \\\\ %\\vspace{0.2cm}\n",
    "\\frac{8}{9} & \\frac{19372}{6561} &-\\frac{25360}{2187}&\n",
    "\\frac{64448}{6561}\n",
    "& -\\frac{212}{729} & & &\\\\ %\\vspace{0.2cm}\n",
    "1 & \\frac{9017}{3168} & -\\frac{355}{33} & \\frac{46732}{5247} &\n",
    "\\frac{49}{176} &\n",
    "-\\frac{5103}{18656} & & \\\\\n",
    "1 & \\frac{35}{84} & 0 & \\frac{500}{1113} & \\frac{125}{192}\n",
    "&-\\frac{2187}{6784} & \\frac{11}{84}  & \\\\ \\hline y_n  &\n",
    "\\frac{35}{384} & 0 & \\frac{500}{1113} & \\frac{125}{192} &\n",
    "-\\frac{2187}{6784} & \\frac{11}{84} & 0\\\\ \\hline \\hat{y}_n &\n",
    "\\frac{5179}{57600} & 0 & \\frac{7571}{16695}& \\frac{393}{640} &\n",
    "-\\frac{92097}{339200} & \\frac{187}{2100} & \\frac{1}{40}\n",
    "\\end{array} \\hspace{0.5cm}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73b796",
   "metadata": {},
   "source": [
    "This method has been optimized for the local extrapolation approach mentioned above, \n",
    "where the highest order method is used to advance the solution and the less accurate method\n",
    "is only used for step size selection. The implementation is otherwise similar to the RKF45 method.\n",
    "The Dormand-Prince method has been implemented in numerous software tools, including the popular\n",
    "`ode45` function in Matlab (The Math Works, Inc. MATLAB. Version 2023a).\n",
    "\n",
    "Implicit RK methods can also be equipped with embedded methods. The fundamental idea is exactly the same as\n",
    "for explicit methods, although the step size selection tends to be more challenging for stiff problems. \n",
    "The most obvious constraint is that for stiff problems, both the main method and the error estimator need to have good\n",
    "stability properties. Stiff problems are also known to be more challenging for the error control algorithms, and\n",
    "simple algorithms such as ([4](#step_size0)) often suffer from large fluctuations in step size and local error. \n",
    "\n",
    "Implicit RK methods can also be equipped with time step control based on embedded error estimators. For instance, the\n",
    "TR-BDF2 method in ([butcher-tr-bdf2](#butcher-tr-bdf2)) can be extended to include a third order method for error estimation. \n",
    "The extended Butcher tableau is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b50b81",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"butcher-tr-bdf23\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{array}{c|ccc}\n",
    "    0 & 0&  & \\\\\n",
    "    2\\gamma & \\gamma & \\gamma & 0\\\\ \n",
    "    1 & \\beta & \\beta & \\gamma \\\\ \\hline\n",
    "     & \\beta & \\beta & \\gamma \\\\\n",
    "     & \\frac{1-\\beta}{3} & \\frac{3\\beta + 1}{3} & \\frac{\\gamma}{3}\n",
    "    \\end{array} ,\\hspace{0.5cm} \n",
    "    \\label{butcher-tr-bdf23} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c3e2a",
   "metadata": {},
   "source": [
    "with $\\gamma = 1-\\sqrt{2}/2$ and $\\beta = \\sqrt{2}/4$, and the bottom line of coefficients defines \n",
    "the third order method. This third order method is not L-stable, and for stiff problems it is\n",
    "therefore preferable to advance the solution using the second-order method and use the more\n",
    "accurate one for time step control. Ideally we would like both methods of an embedded RK pair \n",
    "to be L-stable, but this is often impossible to achieve and we need to accept somewhat\n",
    "weaker stability requirements for the error estimator, see, for instance, \n",
    "[[kvaerno2004singly]](#kvaerno2004singly). \n",
    "\n",
    " When implementing the adaptive TR-BDF2 and other implicit methods, \n",
    "we need to combine features of the `AdaptiveODESolver` class above with the tools \n",
    "from the `ImplicitRK` hierarchy introduced in Chapter 3. Specifically,\n",
    "an adaptive implicit RK methods needs the `solve` and `new_step_size` method \n",
    "from `AdaptiveODESolver`, while all the code for computing the stage derivatives can\n",
    "be reused directly from the `ImplicitRK` classes. A convenient way to reuse\n",
    "functionality from two different classes is to use *multiple inheritance*, where\n",
    "we define a new class a subclass of two different base classes. For instance, a base class\n",
    "for adaptive ESDIRK methods may look like \n",
    "a class may look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c2e61f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdaptiveESDIRK(AdaptiveODESolver,ESDIRK):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458ea69",
   "metadata": {},
   "source": [
    "which simply states that the new class inherits all the methods from both the `AdaptiveODESolver` class\n",
    "and the `ImplicitRK` class. The general design of the `ImplicitRK` class above was to define \n",
    "the method coefficients in the constructor and use a generic `advance` method, and it is convenient to \n",
    "use the same method for the adaptive implicit methods. We then need to override the `advance` method\n",
    "from `ImplicitRK` in our `AdaptiveImplicitRK` base class, since we need the method to return the \n",
    "error in addition to the updated solution. All other methods can be reused directly from either\n",
    "`AdaptiveODESolver` or `ImplicitRK`, so a suitable implementation of the new class may look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56a33ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdaptiveESDIRK(AdaptiveODESolver,ESDIRK):  \n",
    "    def advance(self):\n",
    "        b = self.b\n",
    "        e = self.e\n",
    "        u = self.u\n",
    "        dt = self.dt \n",
    "        k = self.solve_stages()\n",
    "        u_step = dt*sum(b_*k_ for b_,k_ in zip(b,k)) \n",
    "        error = dt*sum(e_*k_ for e_,k_ in zip(e,k))\n",
    "        u_new = u[-1] + u_step\n",
    "        error_norm = np.linalg.norm(error)\n",
    "        return u_new, error_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef336",
   "metadata": {},
   "source": [
    "Here, we assume that the constructor defines all the RK method parameters used earlier, and in addition \n",
    "a set of parameters `self.e`, defined by $e_i = b_i-\\hat{b}_i$, for $i=1,\\ldots, n$, which are used in the\n",
    "error calculations. Except for the two lines computing the error, the method is identical to \n",
    "the generic `advance` method from the `ImplicitRK` class, which was used by all the subclasses. Therefore,\n",
    "it may be natural to ask whether we should have put this method in a general base class for implicit RK\n",
    "methods, for instance named `AdaptiveImplicitRK`, and then it could be used in adaptive versions of both\n",
    "the SDIRK, ESDIRK, and Radau classes. However, adaptive versions of the Radau methods use a slightly \n",
    "different calculation of the error, since for a Radau method of order $p$ it is not possible to \n",
    "construct an embedded method of order $p-1$. For the adaptive solvers the advance method \n",
    "is therefore slightly less general, and it is convenient to implement it separately for the ESDIRK methods. \n",
    "We will not present adaptive versions of the Radau methods here, but the details may be found in \n",
    "[[ODEII]](#ODEII).\n",
    "\n",
    "Although multiple inheritance provides a convenient way to reuse the functionality of our existing classes, \n",
    "it comes with the risk of somewhat complex and confusing class hierarchies. In particular, the fact that\n",
    "our `AdaptiveESDIRK` class inherits from `AdaptiveODESolver` and `ESDIRK`, which are both subclasses\n",
    "of `ODESolver`, may give rise to a well-known ambiguity referred to as the *diamond problem*. \n",
    "The problem would arise if, for instance, we were to define a method in `ODESolver`, override it with special versions in \n",
    "both `AdaptiveODESolver` and `ESDIRK`, and then call it from an instance of `AdaptiveESDIRK`. Would we then\n",
    "call the version implemented in `AdaptiveODESolver` or the one in `ESDIRK`?\n",
    "The answer is determined by Python's so-called *method resolution order* (MRO), which decides \n",
    "which method to inherit first based on its \"closeness\" in the class hierarchy and then on the \n",
    "order of the base classes in the class definition. In our particular example the \n",
    "`AdaptiveESDIRK` class is equally close to `AdaptiveODESolver` and `ESDIRK`, since it is a direct subclass\n",
    "of both. The method called would therefore be the version from `AdaptiveODESolver`, \n",
    "since this is listed first in the class definition. In our relatively simple class hierarchy \n",
    "there are no such ambiguities, and even if we use multiple inheritance it should not be too challenging to \n",
    "determine which methods are called, but it is a potential source of confusion that is worth being aware of. \n",
    "\n",
    "With the `AdaptiveESDIRK` base class available, an adaptive version of the TR-BDF2 method may be \n",
    "implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd240ea6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TR_BDF2_Adaptive(AdaptiveESDIRK):\n",
    "    def __init__(self,f,eta=0.9):\n",
    "        super().__init__(f,eta) #calls AdaptiveODESolver.__init__\n",
    "        self.stages = 3\n",
    "        self.order = 2\n",
    "        gamma = 1-np.sqrt(2)/2\n",
    "        beta = np.sqrt(2)/4\n",
    "        self.gamma = gamma\n",
    "        self.a = np.array([[0,0,0],\n",
    "                            [gamma, gamma,0],\n",
    "                            [beta,beta,gamma]])\n",
    "        self.c = np.array([0,2*gamma,1])\n",
    "        self.b = np.array([beta,beta,gamma])\n",
    "        bh = np.array([(1-beta)/3,(3*beta + 1)/3, gamma/3])\n",
    "        self.e = self.b-bh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5ca0b",
   "metadata": {},
   "source": [
    "To illustrate the use of this solver class, we may return to the Hodgkin-Huxley model considered at the \n",
    "start of this chapter. Assuming that we have implemented the model as a class in a file `hodgkinhuxley.py`, \n",
    "the following code solves the model and plots the transmembrane potential:\n",
    "'' class implementation of this model may look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b78bcf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from AdaptiveImplicitRK import *\n",
    "from hodgkinhuxley import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = HodgkinHuxley()\n",
    "u0 = [-45,0.31,0.05,0.59]\n",
    "t_span = (0,50)\n",
    "tol = 0.01\n",
    "\n",
    "solver = TR_BDF2_Adaptive(model)\n",
    "solver.set_initial_condition(u0)\n",
    "    \n",
    "t,u = solver.solve(t_span,tol)\n",
    "\n",
    "plt.plot(t,u[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b280a36",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [./figs_ch4/hh_adaptive_tr_bdf2.png, width=600 frac=1] Solution of the Hodgkin-Huxley model. The solid line is a reference solution computed with SciPy `solve_ivp`, while the $+$-marks are the time steps chosen by the adaptive TR-BDF2 solver. <div id=\"fig:hh_adap\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:hh_adap\"></div>\n",
    "\n",
    "<p>Solution of the Hodgkin-Huxley model. The solid line is a reference solution computed with SciPy <code>solve_ivp</code>, while the $+$-marks are the time steps chosen by the adaptive TR-BDF2 solver.</p>\n",
    "<img src=\"./figs_ch4/hh_adaptive_tr_bdf2.png\" width=600>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "A plot of the solution is shown in [Figure](#fig:hh_adap). The $+$-marks show the time steps chosen by the \n",
    "adaptive TR-BDF2 solver, and it is easy to see that large time steps are used in quiescent regions while smaller\n",
    "steps are used where the solution varies rapidly. A more quantitative view of the solver behavior, for \n",
    "three different solvers, is shown in the table below. Each method has been applied with three different\n",
    "tolerance values, for the time interval from 0 to 50ms, and with \n",
    "default choices of the maximum and minimum time steps. The column marked \"Error\"\n",
    "is then an estimate of the global error, based on a reference solution computed with SciPy's `solve_ivp`, \n",
    "the \"Steps\" column is the number of accepted time steps, \"Rejected\" is the total number of rejected\n",
    "steps, and the two last columns are the minimum and maximum time steps that occurred during the computation. \n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"left\">Solver \\hspace{1.0cm}</th> <th align=\"center\">Tolerance</th> <th align=\"center\">  Error  </th> <th align=\"center\">$\\hspace{3mm}$Steps $\\hspace{3mm}$</th> <th align=\"center\">Rejected</th> <th align=\"center\">$\\hspace{3mm}\\Delta t_{max}$ \\hspace{3mm}</th> <th align=\"center\"> $\\Delta t_{min}$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"left\">   TR-BDF2                  </td> <td align=\"center\">   1.000        </td> <td align=\"center\">   0.0336961    </td> <td align=\"center\">   24                                    </td> <td align=\"center\">   9           </td> <td align=\"center\">   10.533                                       </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   TR-BDF2                  </td> <td align=\"center\">   0.100        </td> <td align=\"center\">   0.0175664    </td> <td align=\"center\">   43                                    </td> <td align=\"center\">   14          </td> <td align=\"center\">   9.705                                        </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   TR-BDF2                  </td> <td align=\"center\">   0.010        </td> <td align=\"center\">   0.0028838    </td> <td align=\"center\">   83                                    </td> <td align=\"center\">   22          </td> <td align=\"center\">   5.328                                        </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   RKF45                    </td> <td align=\"center\">   1.000        </td> <td align=\"center\">   0.6702536    </td> <td align=\"center\">   192                                   </td> <td align=\"center\">   113         </td> <td align=\"center\">   2.204                                        </td> <td align=\"center\">   $1.0\\cdot10^{-5}$    </td> </tr>\n",
    "<tr><td align=\"left\">   RKF45                    </td> <td align=\"center\">   0.100        </td> <td align=\"center\">   0.0934201    </td> <td align=\"center\">   118                                   </td> <td align=\"center\">   58          </td> <td align=\"center\">   1.093                                        </td> <td align=\"center\">   $1.0\\cdot10^{-5}$    </td> </tr>\n",
    "<tr><td align=\"left\">   RKF45                    </td> <td align=\"center\">   0.010        </td> <td align=\"center\">   0.0054336    </td> <td align=\"center\">   123                                   </td> <td align=\"center\">   34          </td> <td align=\"center\">   1.297                                        </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   EulerHeun                </td> <td align=\"center\">   1.000        </td> <td align=\"center\">   0.7790353    </td> <td align=\"center\">   158                                   </td> <td align=\"center\">   35          </td> <td align=\"center\">   1.849                                        </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   EulerHeun                </td> <td align=\"center\">   0.100        </td> <td align=\"center\">   0.0016577    </td> <td align=\"center\">   220                                   </td> <td align=\"center\">   40          </td> <td align=\"center\">   0.836                                        </td> <td align=\"center\">   0.00791              </td> </tr>\n",
    "<tr><td align=\"left\">   EulerHeun                </td> <td align=\"center\">   0.010        </td> <td align=\"center\">   0.0014654    </td> <td align=\"center\">   432                                   </td> <td align=\"center\">   36          </td> <td align=\"center\">   0.918                                        </td> <td align=\"center\">   0.00251              </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "The numbers in this table illustrate a number of well-known properties and limitations\n",
    "of adaptive ODE solvers. First, we may observe that there is\n",
    "no close relationship between the selected tolerance and the resulting error. \n",
    "The error gets smaller when we reduce the tolerance, and for this particular case\n",
    "the error is always smaller than the specified tolerance, but the error varies\n",
    "substantially between the different methods. As described at the start of this\n",
    "chapter, the time step is selected to control the *local error*, and although we\n",
    "expect the global error to decrease as we reduce the tolerance, we have no \n",
    "guarantee that the global error is lower than the tolerance. A second observation\n",
    "we can make is that the RKF45 and EulerHeun methods both perform rather poorly, \n",
    "and show somewhat inconsistent behavior as the tolerance is reduced. For instance,\n",
    "the RKF45 method uses the largest number of steps, and also rejects the largest number of \n",
    "steps, for the highest tolerance. This behavior is caused by the fact that the \n",
    "Hodgkin-Huxley model is stiff, and the time step for the explicit methods is caused \n",
    "mainly by stability and not accuracy. The $\\Delta t_{min}$ of $1.0\\cdot10^{-5}$ is \n",
    "caused by divergence issues which automatically sets the time step to the specified\n",
    "lower bound. For most of the other combinations of method and tolerance the\n",
    "smallest time step observed is the first one, selected by the simple formula inside \n",
    "the `solve` method. This could obviously be improved, and the general performance of RKF45 \n",
    "for stiff problems could be improved by a sophisticated step size controller. However, \n",
    "the performance will never be on the level of implicit solvers, since the time\n",
    "step will be dictated by stability rather than accuracy.  \n",
    "\n",
    "The ideas and tools introduced in this chapter are fundamental for all RK methods\n",
    "with error control and automatic time step selection. The \n",
    "main ideas are fairly simple, and, as illustrated in [Figure](#fig:hh_adap), give\n",
    "rise to methods that effectively adapt the time step to control the error. There are, however, \n",
    "numerous matters to be considered regarding the practical implementation of the methods, \n",
    "and we have merely scratched the surface of these. As mentioned above, obvious \n",
    "points of improvement include the time step control formula in ([4](#step_size0)), for which\n",
    "more sophisticated models have been derived based on control theory. [[gustafsson1990using]](#gustafsson1990using)\n",
    "The simple formula used for selecting the first time could also be improved, as indicated by the\n",
    "fact that the smallest step $\\Delta t_{min}$ is the first one for most of the solvers in the\n",
    "table above. Furthermore, adjusted error estimates have been proposed, see [[ODEII]](#ODEII), which \n",
    "are more suitable for stiff systems. For a more detailed and complete discussion of automatic\n",
    "time step control, we refer to [[AscherPetzold]](#AscherPetzold) and [[ODEI;@ODEII]](#ODEI;@ODEII)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
