# #if FORMAT == 'ipynb'
========= Programming a simple ODE solver =========
# #endif
idx{ordinary differential equation} 
Ordinary differential equations (ODEs) are widely used in science and engineering, in particular for modeling dynamic
processes. While simple ODEs can be solved with analytical methods, non-linear ODEs are generally not possible
to solve in this way, and we need to apply numerical methods. In this chapter we will see how we can program
general numerical solvers that can be applied to any ODE. We will first consider scalar ODEs, i.e., ODEs with a single
equation and a single unknown, and in Section ref{sec:ode_sys} we will
extend the ideas to systems of coupled ODEs. Understanding the concepts
of this chapter is useful not only for programming your own ODE solvers, but also for using a wide variety of
general-purpose ODE solvers available both in Python and other programming languages.


======= Creating a general-purpose ODE solver =======
label{sec:fe_intro}
When solving ODEs analytically one will typically consider a specific ODE or a class of ODEs, and try to derive
a formula for the solution. In this chapter we want to implement numerical solvers that can be applied to any ODE,
not restricted to a single example or a particular class of equations. For this purpose, we need a general abstract
notation for an arbitrary ODE. We will write the ODEs on the following form:
!bt
\begin{equation}
u^{\prime}(t) = f(t,u(t)),
label{ode0}
\end{equation}
!et
which means that the ODE is fully specified by the definition of the right-hand side function $f(t,u)$. Examples of this
function may be:
!bt
\begin{align*}
f(t,u) &= \alpha u,\quad\hbox{exponential growth}\\
f(t,u) &= \alpha u\left(  1-\frac{u}{R}\right),\quad\hbox{logistic growth}\\
f(t,u) &= -b|u|u + g,\quad\hbox{falling body in a fluid}
\end{align*}
!et
Notice that, for generality, we write all these right-hand sides as functions of both $t$ and $u$, although the
mathematical formulations only involve $u$. This general formulation is not strictly needed in the
mathematical equations, but it is very convenient when we start programming, and want to use the same solver
for a wide range of ODE models. We will discuss this in more detail later. 
Our aim is now to write functions and classes that take $f$ as input, and solve the corresponding ODE to produce
$u$ as output.
idx{exponential growth} idx{logistic growth}

In order for (ref{ode0}) to have a unique solution we need to specify the *initial condition* for $u$, which is 
the value of the solution at time $t=t_0$. The resulting mathematical problem is written as
!bt
\begin{align*}
u^{\prime}(t) &= f(t,u(t)),\\
u(t_0) &= u_0 ,
\end{align*}
!et
and is commonly referred to as an *initial value problem*, or simply an IVP. All the ODE problems we will consider 
in this book are initial value problems. As an example, consider the very simple ODE
!bt
\[ u'=u .\]
!et
This equation has the general solution $u(t)=Ce^t$ for any constant $C$, so it has an infinite number of solutions.
Specifying an initial condition $u(t_0)=u_0$ gives $C=u_0$, and we get the unique solution $u(t)=u_0e^t$.  
We shall see that, when solving the equation numerically, we need to define $u_0$ in order to start our method and
compute a solution at all.

idx{forward Euler method} idx{Euler method !explicit}
=== A simple and general solver; the Forward Euler method. ===
A numerical method for (ref{ode0}) can be derived by simply approximating the derivative
in the equation $u'=f(t,u)$ by a finite difference. To introduce the idea, assume that we have 
already computed $u$ at discrete time points $t_0,t_1,\ldots,t_n$. At time $t_n$ we have the ODE
!bt
\[ u'(t_n) = f(t_n,u(t_n)), \]
!et
and we can now approximate $u'(t_n)$ by a forward finite difference;
!bt
\[ u'(t_n)\approx \frac{u(t_{n+1})-u(t_n)}{\Delta t} .\]
!et
Inserting this approximation into the ODE at $t=t_n$ yields the following equation
!bt
\[ \frac{u(t_{n+1})-u(t_n)}{\Delta t} = f(t_n,u(t_n)), \]
!et
and we can rearrange the terms to obtain an explicit formula for $u(t_{n+1})$:
!bt
\[ u(t_{n+1}) = u(t_n) + \Delta t f(t_n,u(t_n)) .\]
!et
This method is known as the *Forward Euler (FE) method* or the *Explicit Euler Method*,
and is the simplest numerical method for solving an ODE. The classification as a *forward*
or *explicit* method refer to the fact that we have an explicit update formula for $u(t_{n+1})$,
which only involves known quantities at time $t_n$. In contrast, for an *implicit* ODE solver
the update formula will include terms on the form $f(t_{n+1},u(t_{n+1}))$, and we need to solve 
a generally nonlinear equation to determine the unknown $u(t_{n+1})$. We will visit other explicit ODE solvers in
Chapter ref{ch:runge_kutta} and implicit solvers in Chapter ref{ch:stiff}. 

To simplify the formula a bit we introduce the notation $u_{n} = u(t_{n})$, i.e., 
we let $u_n$ denote the numerical approximation to the exact solution $u(t)$ at $t=t_n$.
The update formula now reads 
!bt
\begin{equation} u_{n+1} = u_n + \Delta t f(t_n,u_n) ,
label{forw_euler}
\end{equation}
!et
which, if we know the $u_0$ at time $t_0$, can be applied repeatedly to $u_1$, $u_2$, $u_3$ and so forth.
If we again consider the very simple ODE given by $u' = u$ (i.e., $f(t,u) = u$) we have 
!bt
\begin{align*}
u_1 &= u_0 + \Delta t u_0, \\
u_2 &= u_1 + \Delta t u_1, \\
u_3 &= u_2 + \ldots ,
\end{align*}
!et
and the general update formula
!bt
\[
u_{n+1} = u_n +\Delta t u_n = (1+\Delta t) u_n  .  
\]
!et
In a Python program, the repeated application
of the same formula is conveniently implemented in a for-loop, and the solution can 
be stored in a list or a NumPy array. See, for instance, cite{sundnes2020introduction} for an introduction to
NumPy arrays and tools, which will be used extensively through these notes. 
For a given final time $T$ and number of time steps $N$, 
we perform the following steps:
 o Create arrays `t` and `u` of length $N+1$[^steps_note]
 o Set initial condition: `u[0]` $= u_0$, `t[0]` $= 0$
 o Compute the time step $\Delta t$ `dt = T/N`
 o For $n=0,1,2,\ldots,N-1$:
  * `t[n + 1] = t[n] + dt`
  * `u[n + 1] = (1 + dt) * u[n]`
idx{NumPy array}
# #if FORMAT != 'ipynb'
[^steps_note]: For $N$ time steps, the length of the arrays needs to be $N+1$ since
we need to store both end points, i.e., $t_0,t_1, \ldots, t_n$ and $u_0,u_1, \ldots, u_n$. 
# #endif

A complete Python implementation of this algorithm may look like
!bc pycod
import numpy as np
import matplotlib.pyplot as plt

N = 20
T = 4
dt = T/N
u0 = 1

t = np.zeros(N + 1)
u = np.zeros(N + 1)

u[0] = u0
for n in range(N):
    t[n + 1] = t[n] + dt
    u[n + 1] = (1 + dt) * u[n]

plt.plot(t, u)
plt.show()
!ec
Notice that there is no need to set `t[0]= 0` when `t` is created in this way, but updating `u[0]` is important. In fact, 
forgetting to do so is a very common error in ODE programming, so it is worth taking note of the line `u[0] = u0`.
The solution is shown in Figure ref{fig:ode0}, for two different choices of the time step $\Delta t$. We see that the
approximate solution improves as $\Delta t$ is reduced, although both the solutions are quite inaccurate. However, reducing the
time step further will easily create a solution that cannot be distinguished from the exact solution.

The for-loop in the example above could also be implemented differently, for instance
!bc pycod-t
for n in range(1, N+1):
    t[n] = t[n - 1] + dt
    u[n] = (1 + dt) * u[n - 1]
!ec
Here `n` runs from 1 to `N`, and all the indices inside the loop have been decreased by one so that the
end result is the same. In this simple case it is easy to verify that the two loops give the same result,
but mixing up the two formulations will easily lead to a loop that runs out of bounds (an `IndexError`),
or a loop where the last elements of `t` and `u` are never computed. While seemingly trivial, such errors
are very common when programming with for-loops, and it is a good habit to always examine the loop formulation carefully.

FIGURE: [../chapters/figs_ch1/FE_n_10_20, width=600 frac=1] Solution of $u' = u, u(0) = 1$ with $\Delta t = 0.4$ ($N=10$) and $\Delta t=0.2$ ($N=20$). label{fig:ode0}

=== Extending the solver to the general ODE. ===
As stated above, the purpose of this chapter is to create general-purpose ODE solvers, that can solve any ODE written
on the form $u'=f(t,u)$. This requires a very small modification of the algorithm above;
 o Create arrays `t` and `u` of length $N+1$
 o Set initial condition: `u[0]` = $u_0$, `t[0]=0`
 o For $n=0,1,2,\ldots,N-1$:
  * `t[n + 1] = t[n] + dt`
  * `u[n + 1] = u[n] + dt * f(t[n], u[n])`

The only change of the algorithm is in the formula for computing `u[n+1]` from `u[n]`.
In the previous case we had $f(t,u) = u$, and to create a
general-purpose ODE solver we simply replace `u[n]` with the more general `f(t[n],u[n])`.
The following Python function implements this generic version of the FE method:
!bc pycod
import numpy as np

def forward_euler(f, u0, T, N):
    """Solve u'=f(t, u), u(0)=u0, with n steps until t=T."""
    t = np.zeros(N + 1)
    u = np.zeros(N + 1)  # u[n] is the solution at time t[n]

    u[0] = u0
    dt = T / N

    for n in range(N):
        t[n + 1] = t[n] + dt
        u[n + 1] = u[n] + dt * f(t[n], u[n])

    return t, u
!ec
This simple function can solve any ODE written on the form (ref{ode0}). The right-hand 
side function $f(t,u)$ needs to be implemented as a Python function, which is
then passed as an argument to `forward_euler` together with the initial condition `u0`, the
stop time `T` and the number of time steps `N`. The two latter arguments are then used
to calculate the time step `dt` inside the function.[^source1]

[^source1]: The source code for this function, as well as all subsequent solvers and 
examples, can be found here: 
https://sundnes.github.io/solving\_odes\_in\_python/

To illustrate how the function is used, let us apply it to solve the same problem as above;
$u'=u$, $u(0)=1$, for $t\in [0,4]$. The following code uses the `forward_euler` function to solve this problem:
!bc pycod
def f(t, u):
    return u

u0 = 1
T = 4
N = 30
t, u = forward_euler(f, u0, T, N)
!ec
The `forward_euler` function returns the two arrays `u` and `t`, which we can
plot or process further as we want. One thing worth noticing in this code is the definition of the
right-hand side function `f`. As we mentioned above, this function should always be written
with two arguments `t` and `u`, although in this case only `u` is used inside the function.
The two arguments are needed because we want our solver to work for all ODEs on the
form $u' = f(t,u)$, and the function is therefore called as `f(t[n], u[n])` inside
the `forward_euler` function. If our right
hand side function was defined as a function of `u` only, i.e., using `def f(u):`, 
we would get an error message when the function was called inside `forward_euler`. 
This problem is solved by simply writing `def f(t,u):` even if `t` is never used 
inside the function.[^rhs_note]

# #if FORMAT != 'ipynb'
[^rhs_note]: This way of defining the right-hand side is a standard used by most
available ODE solver libraries, both in Python and other languages. The right-hand
side function always takes two arguments `t` and `u`, but, annoyingly, the order of the
two arguments varies between different solver libraries. Some expect the `t` argument first,
while others expect `u` first.
# #endif

For being only 15 lines of Python code, the capabilities of the `forward_euler`
function above are quite remarkable. Using this function, we can solve any
kind of linear or nonlinear ODE, most of which would be impossible to solve
using analytical techniques. The general recipe for using this function can be summarized
as follows:
 o Identify $f(t,u)$ in your ODE
 o Make sure you have an initial condition $u_0$
 o Implement the $f(t,u)$ formula in a Python function `f(t, u)`
 o Choose the number of time steps `N`
 o Call `t, u = forward_euler(f, u0, T, N)`
 o Plot the solution

It is worth mentioning that the FE method is the simplest of all
ODE solvers, and many will argue that it is not very good. This is partly true,
since there are many other methods that are more accurate and more stable when applied to
challenging ODEs. We shall see later that numerical solutions may not only be inaccurate, 
as illustrated in Figure ref{fig:ode0}, but may also blow up and give completely meaningless results. 
Solvers for avoiding such behavior, i.e., stable solvers, will be presented in Chapter ref{ch:stiff}.
However, the FE method is quite suitable for solving a large number of interesting ODEs.
If we are not happy with the accuracy we can simply reduce the time step, and
in most cases this will give the accuracy we need with a negligible increase in computing time.


======= The ODE solver implemented as a class =======
label{sec:fe_class} 
idx{`ForwardEuler` class}
We can increase the flexibility of the `forward_euler`
solver function by implementing it as a class. There are many ways to implement
such a class, but one possible usage can be as follows:
!bc pycod-t
method = ForwardEuler_v0(f) 
method.set_initial_condition(u0)
t, u = method.solve(t_span=(0, 10), N=100)
plot(t, u)
!ec
idx{class !for ODE solver}
The benefits of using a class instead of a function may not be obvious at this
point, but it will become clear when we introduce different ODE solvers later. 
For now, let us just look at how such a solver class would need to be implemented in 
order to support the use case specified above:
  * We need a constructor (`__init__`) which takes a single argument, the right-hand side function `f`, 
    and stores it as an attribute.
  * The method `set_initial_condition` takes the initial condition as argument and stores it. 
  * The class needs a `solve`-method, which takes the time interval `t_span` and number of time steps `N`
    as arguments. The method implements the for-loop for solving the ODE and returns
    the solution, similar to the `forward_euler` function considered earlier.
  * The time step $\Delta t$ and the sequences $t_n$, $u_n$ must be
    initialized in one of the methods, and it may also be convenient to store these as attributes. 
    Since the time interval and the number of steps are arguments to the `solve` method it is natural
    to do these computations there.

In addition to these methods, it may be convenient to implement the formula for
advancing the solution one step as a separate method `advance`. In this way it
becomes very easy to implement new numerical methods, since we typically only
need to change the `advance` method. A first version of the solver class may
look as follows:
!bc pycod
import numpy as np
class ForwardEuler_v0:
    def __init__(self, f):
        self.f = f  

    def set_initial_condition(self, u0):
        self.u0 = u0

    def solve(self, t_span, N):
        """Compute solution for t_span[0] <= t <= t_span[1],
        using N steps."""
        t0, T = t_span
        self.dt = T / N
        self.t = np.zeros(N + 1)  # N steps ~ N+1 time points
        self.u = np.zeros(N + 1)

        msg = "Please set initial condition before calling solve"
        assert hasattr(self, "u0"), msg

        self.t[0] = t0
        self.u[0] = self.u0

        for n in range(N):
            self.n = n
            self.t[n + 1] = self.t[n] + self.dt
            self.u[n + 1] = self.advance()
        return self.t, self.u

    def advance(self):
        """Advance the solution one time step."""
        # Create local variables to get rid of "self." in
        # the numerical formula
        u, dt, f, n, t = self.u, self.dt, self.f, self.n, self.t

        return u[n] + dt * f(t[n], u[n])
!ec
This class does essentially the same tasks as the `forward_euler` function above,
and the main advantage of the class implementation is
the increased flexibility that comes with the `advance` method. As we shall see
later, implementing a different numerical method typically only requires
implementing a new version of this method, while all other code can be left unchanged.
Notice also that we have added an `assert` statement inside the `solve` method, which 
checks that the user has called `set_initial_condition` before calling `solve`. 
Forgetting to do so is a very likely error for users of the code, and this
`assert` statement ensures that we get a useful error message rather than a 
less informative `AttributeError`. 

We can also use a class to hold the right-hand side $f(t,u)$, which is
particularly convenient for functions with parameters.
Consider for instance the model for logistic growth;
!bt
\[ u^{\prime}(t)=\alpha u(t)\left(  1-\frac{u(t)}{R}\right),\quad u(0)=u_0,\quad t\in [0,40],\]
!et
which is typically used to model self-limiting growth of a biological population, i.e., growth 
that is constrained by limited resources. The initial growth is approximately exponential, with
growth rate $\alpha$, and the population curve flattens out as the population size approaches the *carrying capacity* $R$,
see Figure ref{fig:logistic} for an example solution. 
The right hand side function includes two parameters $\alpha$ and $R$, but if we
want to solve it using our FE function or class, it must be implemented
as a function of $t$ and $u$ only. There are several ways to do this in Python, but one
convenient approach is to implement the function as a class with a call method.[^call_special_meth] 
We can then define the parameters as attributes in the constructor and
use them inside the `__call__` method:
!bc pycod
class Logistic:
    def __init__(self, alpha, R):
        self.alpha, self.R = alpha, float(R)

    def __call__(self, t, u):
        return self.alpha * u * (1 - u / self.R)
!ec
idx{class !for right-hand side}
The main program for solving the logistic growth problem may now look like:
!bc pycod
problem = Logistic(alpha=0.2, R=1.0)
solver = ForwardEuler_v0(problem)
u0 = 0.1
solver.set_initial_condition(u0)

T = 40
t, u = solver.solve(t_span=(0, T), N=400)
!ec
[^call_special_meth]: Recall that if we equip a class with a special method named `__call__`,
instances of the class will be callable and behave like regular Python functions. See, for instance,
Chapter 8 of cite{sundnes2020introduction} for a brief introduction to `__call__` and other special methods. 


FIGURE: [../chapters/figs_ch1/logistic_func_mpl, width=600 frac=1.0] Solution of the logistic growth model. label{fig:logistic}
======= Systems of ODEs =======
label{sec:ode_sys}
idx{systems of odes} idx{ordinary differential equation ! system of}
So far we have only considered ODEs with a single solution component, often called scalar ODEs.
Many interesting processes can be described
by systems of ODEs, i.e., multiple ODEs where the right-hand side of one equation depends on the solution of the others. Such equation
systems are also referred to as vector ODEs. One simple example is
!bt
\begin{alignat*}{2}
u' &= v, \quad &&u(0) = 1\\
v' &= -u, \quad &&v(0) = 0.
\end{alignat*}
!et
The solution of this system is $u=\cos t, v=\sin t$, which can easily be verified by inserting the solution into the equations
and the initial conditions. For more general cases, it is usually even more difficult to find analytical solutions of ODE systems
than of scalar ODEs, and numerical methods are usually required. In this section we will extend the solvers introduced in
sections ref{sec:fe_intro}-ref{sec:fe_class} to be able to solve systems of ODEs. We shall see that such an 
extension requires relatively small modifications of the code.

We want to develop general software that can be applied to any vector ODE or scalar ODE, and for this purpose it is
useful to introduce some general mathematical notation. We have $m$ unknowns
!bt
\[ u^{(0)}(t), u^{(1)}(t), \ldots, u^{(m-1)}(t) \]
!et
in a system of $m$ ODEs:
!bt
\begin{align*}
{d\over dt}u^{(0)} &= f^{(0)}(t, u^{(0)}, u^{(1)}, \ldots, u^{(m-1)}),\\
{d\over dt}u^{(1)} &= f^{(1)}(t, u^{(0)}, u^{(1)}, \ldots, u^{(m-1)}),\\
\vdots &= \vdots\\
{d\over dt}u^{(m-1)} &= f^{(m-1)}(t, u^{(0)}, u^{(1)}, \ldots, u^{(m-1)}).
\end{align*}
!et
To simplify the notation (and later the implementation), we collect both the solutions $u^{(i)}(t)$
and right-hand side functions $f^{(i)}$ into vectors;
!bt
\[ u = (u^{(0)}, u^{(1)}, \ldots, u^{(m-1)}),\]
!et
and
!bt
\[ f = (f^{(0)}, f^{(1)}, \ldots, f^{(m-1)}).\]
!et
Note that $f$ is now a vector-valued function. It takes $m+1$ input arguments ($t$ and the $m$ components of $u$) and returns
a vector of $m$ values.
Using this notation, the ODE system can be written
!bt
\[ 
    u' = f(t, u),\quad u(t_0) = u_0,
\]
!et
where $u$ and $f$ are now vectors and $u_0$ is a vector of initial conditions. We see that we use exactly the
same notation as for scalar ODEs, and whether we solve a scalar or system of ODEs is determined by how we define $f$ and the initial
condition $u_0$. This general notation is completely standard in text books on ODEs, and we can easily make the Python
implementation just as general. The generalization of our ODE solvers is facilitated considerably by the convenience of NumPy
arrays and vectorized computations.

======= A `ForwardEuler` class for systems of ODEs =======
idx{`ForwardEuler` class}
The `ForwardEuler_v0` class above was written for scalar ODEs, and we now want to make it work for a system
$u'=f$, $u(0)=u_0$, where $u$, $f$ and $u_0$ are vectors (arrays). To identify how the code needs to be changed, 
let us first revisit the underlying numerical method. Using the general notation introduced above, 
applying the forward Euler method to a system of ODEs yields an update formula that looks exactly 
as for the scalar case, but where all the terms are vectors:
!bt
\[
\underbrace{u_{k+1}}_{\mbox{vector}} =
\underbrace{u_k}_{\mbox{vector}} +
\Delta t\, \underbrace{f(u_k, t_k)}_{\mbox{vector}} .
\]
!et
We could also write this formula in terms of the individual components, as in
!bt
\[
u^{(i)}_{k+1} = u^{(i)}_{k} + \Delta t f^{(i)}(t_k, u_{k}), \mbox{ for } i = 0,\ldots , {m-1},
\]
!et
but the compact vector notation is much easier to read. Fortunately, the way we write the vector
version of the formula is also how NumPy arrays are used in calculations. The
Python code for the formula above may therefore look identical to the version for scalar ODEs;
!bc pycod
u[k + 1] = u[k] + dt * f(t[k], u[k])
!ec
with the important difference that both `u[k]`, `u[k+1]`, and `f(t[k], u[k])` are now arrays.[^numpy]
Since these are arrays, the solution `u` must be a
two-dimensional array, and `u[k],u[k+1]`, etc. are the rows of this array.
The function `f` expects an array as its second argument, and must return a one-dimensional array,
containing all the right-hand sides $f^{(0)},\ldots,f^{(n-1)}$. To get a better
feel for how these arrays look and how they are used,
we may compare the array holding the solution of a scalar ODE to that of a system of two ODEs.
For the scalar equation, both `t` and `u` are one-dimensional NumPy
arrays, and indexing into `u` gives us numbers, representing the solution at each time step.
For instance, in an interactive Python session we may have arrays `t` and `u` 
with the following contents:
!bc
>>> t
array([0. ,  0.4, 0.8, 1.2, ... ])
>>> u
array([1. , 1.4, 1.96, 2.744, ... ])
!ec
and indexing into `u` then gives
!bc
>>> u[0]
1.0
>>> u[1] 
1.4
!ec
In the case of a system of two ODEs, `t` is still a one-dimensional array, but the solution array `u` is
now two-dimensional, with one column for each solution component. We can index it exactly as shown above, 
and the result is a one-dimensional array of length two, which holds the two solution components
at a single time step:
!bc
>>> u 
array([[1.0, 0.8],
        [1.4, 1.1],
        [1.9, 2.7],
        ... ])

>>> u[0]
array([1.0, 0.8])
>>> u[1]
array([1.4, 1.1])
!ec
Equivalently, we could write
!bc
>>> u[0,:]
array([1.0, 0.8])
>>> u[1,:]
array([1.4, 1.1])
!ec
to make explicit which of the two array dimensions (or *axes*) that we are indexing into. 
# #if FORMAT  != 'ipynb'
[^numpy]: This compact notation requires that the solution vector `u` is represented by
a NumPy array. We could, in principle, use lists to hold the solution components, but
the resulting code would need to loop over the components and would be far less elegant and readable.
# #endif

The similarity of the generic mathematical notation for vector and scalar ODEs, and the
convenient algebra of NumPy arrays, indicate that the solver
implementation for scalar and system ODEs can also be very similar. This is indeed true,
and the `ForwardEuler_v0` class from the previous chapter can be made to work for ODE
systems by a few minor modifications:
 * Ensure that `f(t,u)` always returns an array.
 * Inspect the initial condition `u0` to see if it is a single number (scalar) or a list/array/tuple, and
   make the array `u` either a one-dimensional or two-dimensional array.[^u0_scalar] 
If these two items are handled and initialized correctly, the rest of the code from
Section ref{sec:fe_class} will in fact work with no modifications. 

[^u0_scalar]: This step is not strictly needed, since we could use a two-dimensional array with
shape `(N + 1, 1)` for scalar ODEs. However, using a one-dimensional array for scalar ODEs gives 
simpler and more intuitive indexing. 

The extended class implementation may look like:
!bc pycod
import numpy as np

class ForwardEuler:
    def __init__(self, f):
        self.f = lambda t, u: np.asarray(f(t, u), float)

    def set_initial_condition(self, u0):
        if np.isscalar(u0):              # scalar ODE
            self.neq = 1                 # no of equations
            u0 = float(u0)
        else:                            # system of ODEs
            self.neq = u0.size           # no of equations
            u0 = np.asarray(u0)           
        self.u0 = u0

    def solve(self, t_span, N):
        """Compute solution for
        t_span[0] <= t <= t_span[1],
        using N steps."""
        t0, T = t_span
        self.dt = (T - t0) / N
        self.t = np.zeros(N + 1)
        if self.neq == 1:
            self.u = np.zeros(N + 1)
        else:
            self.u = np.zeros((N + 1, self.neq))

        msg = "Please set initial condition before calling solve"
        assert hasattr(self, "u0"), msg

        self.t[0] = t0
        self.u[0] = self.u0

        for n in range(N):
            self.n = n
            self.t[n + 1] = self.t[n] + self.dt
            self.u[n + 1] = self.advance()
        return self.t, self.u

    def advance(self):
        """Advance the solution one time step."""
        u, dt, f, n, t = self.u, self.dt, self.f, self.n, self.t
        return  u[n] + dt * f(t[n], u[n])

!ec
It is worth commenting on some parts of this code. First, the constructor looks
almost identical to the scalar case, but we use a lambda function and
the convenient `np.asarray` function to convert any `f` that 
returns a list or tuple to a function returning a NumPy array. If `f` already returns an
array, `np.asarray` will simply return this array with no changes. This modification is not strictly
needed, since we could just assume that the user implements `f` to return
an array, but it makes the class more robust and flexible. We have also used 
the function `isscalar` from NumPy in the `set_initial_condition` method, to check if `u0` is 
a single number or a NumPy array, and define the attribute `self.neq` to
hold the number of equations.
The final modification is found in the method `solve`, where
the `self.neq` attribute is inspected and `u` is
initialized to a one- or two-dimensional array of the correct size. The
actual for-loop and the `advance` method are both identical to the previous version of the class.

=== Example: ODE model for a pendulum. ===
idx{pendulum problem}
To demonstrate the use of
the updated `ForwardEuler` class, we consider a system of ODEs describing the motion 
of a simple pendulum, as illustrated in Figure ref{fig:pendulum}. This nonlinear system
is a classic physics problem, and despite its simplicity it is not possible to find 
an exact analytical solution. We will formulate the system in terms of two main
variables; the angle $\theta$ and the angular velocity $\omega$, see Figure ref{fig:pendulum}.
For a simple pendulum with no friction, the dynamics of these two variables is governed by
!bt
\begin{align}
\frac{d \theta}{dt} &= \omega, label{pendulum1} \\
\frac{d \omega}{dt} &= -\frac{g}{L}\sin(\theta), label{pendulum2}  
\end{align}
!et
where $L$ is the length of the pendulum and $g$ is the gravitational constant. Eq. (ref{pendulum1}) follows
directly from the definition of the angular velocity, while (ref{pendulum2}) follows from Newton's second 
law, where $d\omega/dt$ is the acceleration and the right-hand side is the tangential component of
the gravitational force acting on the pendulum, divided by its mass.  
To solve the system we need to define initial conditions for both unknowns,
i.e., we need to know the initial position and velocity of the pendulum. 

FIGURE: [../chapters/figs_ch1/pendulum.png, width=100 frac=0.2] Illustration of the pendulum problem. The main variables of interest are the angle $\theta$ and its derivative $\omega$ (the angular velocity). label{fig:pendulum}

Since the right-hand side defined by (ref{pendulum1})-(ref{pendulum2}) includes the 
parameters $L$ and $g$, it is convenient to implement it as a class, as illustrated for the logistic
growth model earlier. A possible implementation may look like this:
!bc pycod
from math import sin

class Pendulum:
    def __init__(self, L, g=9.81):
        self.L = L
        self.g = g

    def __call__(self, t, u):
        theta, omega = u
        dtheta = omega
        domega = -self.g / self.L * sin(theta)
        return [dtheta, domega]
!ec
We see that the function returns a list, but this will automatically
be wrapped into a function returning an array by the solver class' constructor,
as mentioned above. The main program is not very different
from the examples of the previous chapter, except that we need to define an
initial condition with two components. Assuming that this class definition as 
well as the `ForwardEuler` exist in the same file, the code to solve the pendulum problem
can look like this: 
!bc pycod
import matplotlib.pyplot as plt

problem = Pendulum(L=1)
solver = ForwardEuler(problem)
solver.set_initial_condition([np.pi / 4, 0])
T = 10
N = 1000
t, u = solver.solve(t_span=(0, T), N=N)

plt.plot(t, u[:, 0], label=r'$\theta$')
plt.plot(t, u[:, 1], label=r'$\omega$')
plt.xlabel('t')
plt.ylabel(r'Angle ($\theta$) and angular velocity ($\omega$)')
plt.legend()
plt.show()
!ec
Notice that to extract each solution component we need to index into the 
second index of `u`, using array slicing. Indexing with the first index, for instance using `u[0]` or 
`u[0,:]`, would give us an array of length two that contains the solution components at the first time point. 
In this specific example a call like `plt.plot(t, u)` would also work, and would plot both solution
components. However, we are often interested in plotting selected components of the solution, and
in this case the array slicing is needed. The resulting plot is shown in Figure ref{fig:pendulum_sol}. 
Another minor detail worth noticing is use of Python's raw string format for the labels, indicated
by the `r` in front of the string. Raw strings will treat the backslash (`\`) as a regular character, and is
often needed when using Latex encoding of mathematical symbols. 
The observant reader may also notice that the amplitude of the pendulum motion appears to increase over time, 
which is clearly not physically correct. In fact, for an undamped pendulum problem defined by 
(ref{pendulum1})-(ref{pendulum2}), the energy is conserved, and the amplitude should therefore be constant. 
The increasing amplitude is a numerical artefact introduced by the forward Euler method, and the solution
may be improved by reducing the time step or replacing the numerical method. 

FIGURE: [../chapters/figs_ch1/pendulum_FE.pdf, width=600 frac=1] Solution of the simple pendulum problem, computed with the forward Euler method. label{fig:pendulum_sol}


======= Checking the error in the numerical solution =======
label{sec:error_taylor}
idx{error analysis} idx{Taylor expansion} idx{convergence} 
Recall from Section ref{sec:fe_intro} that we derived the FE method by approximating the
derivative with a finite difference;
!bt
\begin{equation} 
    u'(t_n)\approx \frac{u(t_{n+1})-u(t_n)}{\Delta t} .
\label{fd_approx}\end{equation}
!et
This approximation obviously introduces an error, and since we approach the true derivative as $\Delta t\rightarrow 0$ it
is quite intuitive that the error depends on the size of the time step $\Delta t$. This relation was demonstrated
visually in Figure ref{fig:ode0}, but it would of course be valuable to have a more precise quantification of how the
error depends on the time step. Analyzing the error in numerical methods is a large branch of applied mathematics, 
which we will not cover in detail here, and the interested reader is referred to, for instance cite{ODEI}. 
However, when implementing a numerical method it is very useful to know its theoretical accuracy, and
in particular to be able to compute the error and verify that it behaves as expected.

The Taylor expansion, also discussed briefly in Appendix ref{sec:diffeq_taylor}, is an essential tool for 
estimating the error in numerical methods for ODEs. For any smooth function $\hat{u}(t)$, if we 
can compute the function value and its derivatives at $t_n$, the value at $t_n+\Delta t$ 
can be approximated by the series
!bt
\[
    \hat{u}(t_n+\Delta t) = \hat{u}(t_n) + \Delta t \hat{u}'(t_n) +\frac{\Delta t^2}{2}\hat{u}''(t_n) + 
    \frac{\Delta t^3}{6}\hat{u}'''(t_n)+O(\Delta t^4).  
\]
!et
We can include as many terms as we like, and since $\Delta t$ is small we always have $\Delta t^{(n+1)} \ll \Delta t^n$, so
the error in the approximation is dominated by the first neglected term.  
The update formula of the FE method, derived from (ref{fd_approx}), was
!bt
\[
 u_{n+1} = u(t_n) + \Delta t u'(t_n),
\]
!et
which we may recognize as a Taylor series truncated after the first order term, and we expect the error
$|u_{n+1}- \hat{u}_{n+1}|$ to be proportional to $\Delta x^2$. Since this is the error for a single time step, the
accumulated error after $N\sim 1/\Delta t $ steps is proportional to $\Delta t$, and the 
FE method is hence a \emph{first order} method. As we will see in Chapter 
ref{ch:runge_kutta}, more accurate methods can be constructed by deriving update formulas that make more
terms in the Taylor expansion of the error cancel. This process is fairly straightforward for low-order methods, 
e.g., of second or third order, but it quickly gets complicated for high order solvers, see, for 
instance cite{ODEI} for details. 

Knowing the theoretical accuracy of an ODE solver is important for a number of reasons, and one of them is that it
provides a method for verifying our implementation of the solver. If we can solve a given problem and demonstrate that
the error behaves as predicted by the theory, it gives a good indication that the solver is implemented correctly. 
We can illustrate this procedure using the simple initial value problem introduced earlier;
!bt
\[
    u' = u, \quad u(0) = 1 .
\]
!et
As stated above, this problem has the analytical solution $u=e^t$, and we can use this to compute the 
error in our numerical solution. But how should the error be defined?  
There is no unique answer to this question. For practical applications,
the so-called root-mean-square (RMS) or relative-root-mean-square (RRMS) are commonly used error measures, defined by
!bt
\begin{align*}
RMSE &=  \sqrt{\frac{1}{N}\sum_{n=0}^N(u_n-\hat{u}(t_n))^2}, \\
RRMSE &= \sqrt{\frac{1}{N}\sum_{n=0}^N\frac{(u_n-\hat{u}(t_n))^2}{\hat{u}(t_n)^2}}, 
\end{align*}
!et
respectively. Here, $u_n$ is the numerical solution at time step $n$ and $\hat{u}(t_n)$ the corresponding
exact solution. In more mathematically oriented texts, the errors are usually defined in terms of \emph{norms}, for 
instance the discrete $l_1,l_2,$ and $l_\infty$ norms:
!bt
\begin{align*}
 e_{l_1} &= \sum_{i=0}^N(|u_i-\hat{u}(t_i)|),\\
 e_{l_2} &= \sum_{i=0}^N(u_i-\hat{u}(t_i)^2), \\
 e_{l_\infty} &= \max_{i=0}^N(\hat{u}_i-u(t_i)).  
\end{align*}
!et
While the choice of error norm may be important for certain cases, for practical
applications it is usually not very important, and all the different error measures can be expected to behave
as predicted by the theory. For simplicity, we will apply an even simpler error measure for our example, where
we simply compute the error at the final time $T$, given by $e = |u_N-\hat{u}(t_N)|$. Using the `ForwardEuler`
class introduced above, the complete code for checking the convergence may look as follows:
!bc pycod
from forward_euler_class_v1 import ForwardEuler
import numpy as np

def rhs(t, u):
    return u

def exact(t):
    return np.exp(t)

solver = ForwardEuler(rhs)
solver.set_initial_condition(1.0)

T = 3.0
t_span = (0,T)
N = 30

print('Time step (dt)   Error (e)      e/dt')
for _ in range(10):
    t, u = solver.solve(t_span, N)
    dt = T / N
    e = abs(u[-1] - exact(T))
    print(f'{dt:<14.7f}   {e:<12.7f}   {e/dt:5.4f}')
    N = N * 2     
!ec 
Most of the lines are identical to the previous programs, but we have put the call to the `solve` method inside
a for loop, and the last line ensures that the number of time steps `N` is doubled for each iteration of the
loop. Notice also the f-string format specifiers, for instance `{dt:<14.7f}`, which sets the output to be 
a left aligned decimal number with seven decimals, and occupying 14 characters in total. The purpose of these
specifiers is to output the numbers as vertically aligned columns, which improves readability and 
may be important for visually inspecting the convergence. See, for instance, cite{sundnes2020introduction} for
a brief introduction to f-strings and format specifiers. 
The program will produce the following output:
!bc dat
Time step (dt)   Error (e)      e/dt
0.1000000        2.6361347      26.3613
0.0500000        1.4063510      28.1270
0.0250000        0.7273871      29.0955
0.0125000        0.3700434      29.6035
0.0062500        0.1866483      29.8637
0.0031250        0.0937359      29.9955
0.0015625        0.0469715      30.0618
0.0007813        0.0235117      30.0950
0.0003906        0.0117624      30.1116
0.0001953        0.0058828      30.1200
!ec
In the rightmost column we see that the error divided by the time step is approximately constant, which
supports the theoretical result that the error is proportional to $\Delta t$. In subsequent chapters we will
perform similar calculations for higher order methods, to confirm that the error is proportional to $\Delta t^r$,
where $r$ is the theoretical order of convergence for the method. 

In order to compute the error in our numerical solution we obviously need to know the true solution to our 
initial value problem. This part was easy for the simple example above, since we knew the analytical solution
to the equation, but this solution is only available for very simple ODE problems. It is, however, often 
interesting to estimate the error and the order of convergence for more complex problems, and for this task we 
need to take a different approach. Several alternatives exist, including for instance the *method of manufactured
solutions*, where one simply choses a solution function $u(t)$ and computes its derivative analytically to 
determine the right-hand side of the ODE. An even simpler approach, which usually works well, is to 
compute a very accurate numerical solution using a high-order solver and small time steps, and use this
solution as the reference for computing the error. For accurate error estimates it is of course essential
that the reference solution is considerably more accurate than the numerical solution we want to evaluate. 
The reference solution therefore typically requires very small time steps and can take some time to 
compute, but in most cases the computation time will not be a problem. 


======= Using ODE solvers from SciPy =======
idx{SciPy} idx{`solve_ivp`}
As mentioned in the preface to this book, there are numerous ODE solvers around that can 
be used directly, so one may argue that there is no need to implement our own solvers. 
This may indeed be true, but, as we have argued earlier, it is sometimes very useful to 
know the inner workings of the solvers we apply, and the best way to obtain this 
knowledge is to implement the solvers ourselves. However, if we have a given ODE model and
want to solve it as efficiently as possible, there are several existing solvers to choose from.
For Python programmers, the most natural choice may be the solvers from *SciPy*, which 
have evolved into a robust and fairly efficient suite of ODE solvers. SciPy is a large suite 
of scientific software in Python, including tools for linear algebra, optimization, integration,
and other common tasks of scientific computing.[^scipy] For solving initial 
value problems, the tool of choice is the `solve_ivp` function from the `integrate` module. 
The following code applies `solve_ivp` with the `Pendulum` class presented above to solve the
simple pendulum problem defined by (ref{pendulum1})-(ref{pendulum2}). We assume that the `Pendulum`
class is saved in a separate file `pendulum.py`.
!bc pycod
from scipy.integrate import solve_ivp
import numpy as np
import matplotlib.pyplot as plt
from pendulum import Pendulum

problem = Pendulum(L = 1)
t_span = (0, 10.0)
u0 = (np.pi/4, 0)

solution = solve_ivp(problem, t_span, u0)

plt.plot(solution.t, solution.y[0,:])
plt.plot(solution.t, solution.y[1,:])
plt.legend([r'$\theta$',r'$\omega$'])
plt.show()
!ec
# #if FORMAT != 'ipynb'
[^scipy]: See https://scipy.org/
# #endif


FIGURE: [../chapters/figs_ch1/pendulum_scipy.pdf, width=600 frac=1] Solution of the simple pendulum problem, computed with the SciPy `solve_ivp` function and the default tolerance. label{fig:pendulum_scipy}

Running this code will result in a plot similar to Figure ref{fig:pendulum_scipy}, and we observe that
the solution does not look nearly as nice as the one we got from the `ForwardEuler` solver above. 
The reason for this apparent error is that `solve_ivp` is an *adaptive* solver, which chooses the 
time step automatically to satisfy a given error tolerance. The default value of this tolerance is relatively large, 
which results in the solver using very few time steps and the solution plots looking jagged. 
If we compare the plot with a very accurate numerical solution, indicated by the two dotted curves in
Figure ref{fig:pendulum_scipy}, we see that the solution at the time points $t_n$ 
is quite accurate, but the linear interpolation between the time points completely destroys the visual 
appearance. A more visually appealing solution can be obtained in several ways. 
We may, for instance, pass the function an additional argument `t_eval`, which is a NumPy array 
containing the time points where we want to evaluate the solution: 
!bc pycod
t_eval = np.linspace(0, 10.0, 1001)
solution = solve_ivp(problem, t_span, u0,t_eval=t_eval)
!ec
Alternatively, we can reduce the error tolerance of the solver, for instance setting
!bc pycod
rtol = 1e-6
solution = solve_ivp(problem, t_span, u0, rtol=rtol)
!ec
This latter call will reduce the relative tolerance `rtol` from its default value of `1e-3` (0.001). 
We could also adjust the absolute tolerance using the parameter `atol`. While we will not
consider all the possible arguments and options to `solve_ivp` here, we mention that we can also 
change the numerical method used by the function, by passing in a parameter named `method`. For instance, 
a call like 
!bc pycod
rtol = 1e-6
solution = solve_ivp(problem, t_span, u0, method='Radau')
!ec
will replace the default solver (called `rk45`) with an implicit Radau ODE solver, which we will introduce and
explain in Chapter ref{ch:stiff}. For a complete description of parameters accepted by the `solve_ivp` 
function we refer to the online SciPy documentation.


