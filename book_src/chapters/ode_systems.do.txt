======= Solving systems of ODEs =======
So far we have only considered ODEs with a single solution component, often called scalar ODEs.
Many interesting processes can be described
by systems of ODEs, i.e., multiple ODEs where the right hand side of one equation depends on the solution of the others. Such equation
systems are also referred to as vector ODEs. One simple example is
!bt
\begin{alignat*}{2}
u' &= v, \quad &&u(0) = 1\\
v' &= -u, \quad &&v(0) = 0.
\end{alignat*}
!et
The solution of this system is $u=\cos t, v=\sin t$, which can easily be verified by insterting the solution into the equations
and initial conditions. For more general cases, it is usually even more difficult to find analytical solutions of ODE systems
than of scalar ODEs, and numerical methods are usually required. In this section we will extend the solvers introduced in
sections ref{sec:fe_intro}-ref{sec:fe_class} to be able to solve systems of ODEs. We shall see that such an 
extension requires relatively small modifications of the code.

We want to develop general software that can be applied to any vector ODE or scalar ODE, and for this purpose it is
useful to introduce general mathematical notation. We have $n$ unknowns
!bt
\[ u^{(0)}(t), u^{(1)}(t), \ldots, u^{(n-1)}(t) \]
!et
in a system of $n$ ODEs:
!bt
\begin{align*}
{d\over dt}u^{(0)} &= f^{(0)}(u^{(0)}, u^{(1)}, \ldots, u^{(n-1)}, t),\\
{d\over dt}u^{(1)} &= f^{(1)}(u^{(0)}, u^{(1)}, \ldots, u^{(n-1)}, t),\\
\vdots &= \vdots\\
{d\over dt}u^{(n-1)} &= f^{(n-1)}(u^{(0)}, u^{(1)}, \ldots, u^{(n-1)}, t).
\end{align*}
!et
To simplify the notation (and later the implementation), we collect both the solutions $u^{(i)}(t)$
and right-hand side functions $f^{(i)}$ in vectors;
!bt
\[ u = (u^{(0)}, u^{(1)}, \ldots, u^{(n-1)}),\]
!et
and
!bt
\[ f = (f^{(0)}, f^{(1)}, \ldots, f^{(n-1)}).\]
!et
Note that $f$ is now a vector-valued function. It takes $n+1$ input arguments ($t$ and the $n$ components of $u$) and returns
a vector of $n$ values.
The ODE system can now be written
!bt
\[ u' = f(u, t),\quad u(0) = u_0\]
!et
where $u$ and $f$ are vectors and $u_0$ is a vector of initial conditions. We see that we use exactly the
same notation as for scalar ODEs, and whether we solve a scalar or system of ODEs is determined by how we define $f$ and the initial
condition $u_0$. This general notation is completely standard in text books on ODEs, and we can easily make the Python
implementation just as general. The generalization of our ODE solvers is facilitated considerably by the convenience of NumPy
arrays and vectorized computations.

======= A `ForwardEuler` class for systems of ODEs =======
The `ForwardEuler` classes above were written for scalar ODEs, and we now want to make it work for a system
$u'=f$, $u(0)=U_0$, where $u$, $f$ and $U_0$ are vectors (arrays). To identify how the code needs to be changed, 
let us first revisit the underlying numerical method. Applying the forward Euler method to a system of ODEs yields an update formula that
looks exactly as for the scalar case, but where all the terms are vectors:
!bt
\[
\underbrace{u_{k+1}}_{\mbox{vector}} =
\underbrace{u_k}_{\mbox{vector}} +
\Delta t\, \underbrace{f(u_k, t_k)}_{\mbox{vector}} .
\]
!et
We could also write this formula in terms of the individual components, as in
!bt
\[
u^{(i)}_{k+1} = u^{(i)}_{k} + \Delta t f^{(i)}(u_{k},t_k), \mbox{ for } i = 0,\ldots , {n-1},
\]
!et
but the compact vector notation is much easier to read. Fortunately, the way we write the vector
version of the formula is also how NumPy arrays are used in calculations. The
Python code for the formula above may therefore look idential to the version for scalar ODEs;
!bc pycod
u[k+1] = u[k] + dt*f(u[k], t)
!ec
with the important difference that both `u[k]` and `u[k+1]` are now arrays.[^numpy]
Since these are arrays, the solution `u` must be a
two-dimensional array, and `u[k],u[k+1]`, etc. are the rows of this array.
The function `f` expects an array as its first argument, and must return a one-dimensional array,
containing all the right-hand sides $f^{(0)},\ldots,f^{(n-1)}$. To get a better
feel for how these arrays look and how they are used,
we may compare the array holding the solution of a scalar ODE to that of a system of two ODEs.
For the scalar equation, both `t` and `u` are one-dimensional NumPy
arrays, and indexing into `u` gives us numbers, representing the solution at each time step: 
!bc
t = array([0. ,  0.4, 0.8, 1.2, (...) ])

u = array([ 1.0 1.4  1.96 2.744  (...)])

u[0] = 1.0
u[1] = 1.4

(...)
!ec
In the case of a system of two ODEs, `t` is still a one-dimensional array, but the solution array `u` is
now two-dimensional, with one column for each solution component. Indexing into it
yields one-dimensional arrays of length two, which are the two solution components
at each time step:
!bc
u = array([[1.0, 0.8],
           [1.4, 1.1],
           [1.9, 2.7],
              (...)])

u[0] = array([1.0, 0.8])
u[1] = array([1.4, 1.1])

(...)
!ec

# #if FORMAT  != 'ipynb'
[^numpy]: This compact notation requires that the solution vector `u` is represented by
a NumPy array. We could also, in principle, use lists to hold the solution components, but
the resulting code would need to loop over the components and would be far less elegant and readable.
# #endif

The similarity of the generic notation for vector and scalar ODEs, and the
convenient algebra of NumPy arrays, indicate that the solver
implementation for scalar and system ODEs can also be very similar. This is indeed true,
and the `ForwardEuler_v2` class from the previous chapter can be made to work for ODE
systems by a few minor modifactions:
 * Ensure that `f(u,t)` always returns an array.
 * Inspect $U_0$ to see if it is a single number or a list/array/tuple and
   make the `u` either a one-dimensional or two-dimensional  array
If these two items are handled and initialized correctly, the rest of the code from
Section ref{sec:fe_class} will in fact work with no modifications.
The extended superclass implementation may look like:
!bc pycod
import numpy as np

class ForwardEuler:
    def __init__(self, f):
        # Wrap user's f in a new function that always
        # converts list/tuple to array (or let array be array)
        self.f = lambda u, t: np.asarray(f(u, t), float)

    def set_initial_condition(self, U0):
        if isinstance(U0, (float,int)):  # scalar ODE
            self.neq = 1                 # no of equations
            U0 = float(U0)
        else:                            # system of ODEs
            U0 = np.asarray(U0)
            self.neq = U0.size           # no of equations
        self.U0 = U0

    def solve(self, time_points):
        self.t = np.asarray(time_points)
        N = len(self.t)
        if self.neq == 1:  # scalar ODEs
            self.u = np.zeros(N)
        else:              # systems of ODEs
            self.u = np.zeros((N,self.neq))

        # Assume that self.t[0] corresponds to self.U0
        self.u[0] = self.U0

        # Time loop
        for n in range(N-1):
            self.n = n
            self.u[n+1] = self.advance()
        return self.u, self.t
!ec
It is worth commenting on some parts of this code. First, the constructor looks
almost identical to the scalar case, but we use a lambda function and
`np.asarray` to convert any `f` that returns a list or tuple to a function
returning a NumPy array. This modification is not strictly
needed, since we could just assume that the user implements `f` to return
an array, but it makes the class more robust and flexible. We have also included
tests in the `set_initial_condition` method, to check if `U0` is a single
number (`float`) or a NumPy array, and define the attribute `self.neq` to
hold the number of equations.
The final modification is found in the method `solve`, where
the `self.neq` attribute is inspected and `u` is
initialized to a one- or two-dimensional array of the correct size. The
actual for-loop, as well as the implementation of the `advance` method in the
subclasses, can be left unchanged.

=== Example: ODE model for a pendulum. ===
To demonstrate the use of
the updated `ForwardEuler` class, we consider a system of ODEs describing the motion 
of a simple pendulum, as illustrated in Figure ref{fig:pendulum}. This nonlinear system
is a classic physics problem, and despite its simplicity it is not possible to find 
an exact analytical solution. We will formulate the system in terms of two main
variables; the angle $\theta$ and the angle velocity $\omega$, see Figure ref{fig:pendulum}.
For a simple pendulum with no friction, the dynamcs of these two variables is governed by
!bt
\begin{align}
\frac{d \theta}{dt} &= \omega, label{pendulum1} \\
\frac{d \omega}{dt} &= -\frac{g}{L}\sin(\theta), label{pendulum2}  
\end{align}
!et
where $L$ is the length of the pendulum and $g$ is the gravitational constant. Eq. (ref{pendulum1}) follows
directly from the definition of the angular velocity, while (ref{pendulum2}) follows from Newton's second 
law, where $d\omega/dt$ is the acceleration and the right hand side is the tangential component of
the gravitational force acting on the pendulum, divided by its mass.  
To solve the system we need to define initial conditions for both unknowns,
i.e., we need to know the initial position and velocity of the pendulum. 

FIGURE: [../chapters/fig-ode2/pendulum.png, width=800 frac=0.8] Illustration of the pendulum problem. The main variable of interest is the angle $\theta$ and the angular velocity $\omega$. label{fig:pendulum}

Since the right hand side defined by (ref{pendulum1})-(ref{pendulum2}) includes the 
parameters $L$ and $g$, it is convenient to implement it as a class, as illustrated for the logistic
growth model earlier. A possible implementation may look like this:
!bc pycod
from math import sin

class Pendulum
    def __init__(self, L, g = 9.81):
        self.L = L 
        self.g = g 
    
    def __call__(u, t):
        theta, omega = u
        dtheta = omega
        domega = -self.g/self.L * sin(theta)
        return [dtheta, domega]
!ec
We see that the function returns a list, but this will automatically
be wrapped into a function returning an array by the solver class' constructor,
as mentioned above. The main program is not very different
from the examples of the previous chapter, except that we need to define an
initial condition with two components. Assuming that this class definition as 
well as the `ForwardEuler` exist in the same file, the code to solve the pendulum problem
can look like this: 
!bc pycod
import numpy as np
import matplotlib.pyplot as plt

# Initial condition:
theta0, omega0 = np.pi/4, 0

pendulum = Pendulum(L=1)

solver= ForwardEuler(pendulum)
solver.set_initial_condition((theta0,omega0))
time_points = np.linspace(0, 10.0, 101)
u, t = solver.solve(time_points)
# u is an array of [theta,omega] arrays, we plot both:
plt.plot(x, u[:,0],label=r'$\theta$')
plt.plot(x, u[:,1],label=r'$\omega$')
plt.legend()
plt.show()
!ec
Notice that since `u` is a two-dimensional array, we use array slicing to extract and plot the individual components.
In this specific example a call like `plt.plot(t,u)` would also work, and would plot both solution
components. However, we are often interested in plotting selected components of the solution, and
in this case the array slicing is needed. The resulting plot is shown in Figure ref{fig:pendulum_sol}.

======= Checking the error in the numerical solution =======
Recall from Section ref{sec:ode_intro} that we derived the FE method by approximating the
derivative with a finite difference;
!bt
\[ 
    u'(t_n)\approx \frac{u(t_{n+1})-u(t_n)}{\Delta t} .
\]
!et
This approximation obviously introduces an error, and since we approach the true derivative as $\Delta t\rightarrow 0$ it
is quite intuitive that the error depends on the size of the time step $\Delta t$. This relation was demonstrated
visually in Figure ref{fig:ode0}, but it would of course be valuable to have a more precise quantification of how the
error depends on the time step. Analyzing the error in numerical methods is a large branch of applied mathematics, 
and we will not dive into the details of the field here (NBNB cite ODE books). It is, however, very useful to have an idea
of the accuracy of the methods applied, and in particular to be able to calculate the error and verify that it behaves
as expected.

The error of the FE method can be estimated by consider a Taylor expansion of the solution $u$ around the time $t_n$. 
(NBNB ref appendix for the Taylor expansion? and ODE textbook?) 
For a small time step $\Delta t$, we have (NBNB check notation)
!bt
\[
    u(t_n+\Delta t) = u(t_n) + \Delta t \frac{du}{dt}(t_n) +\frac{1}{2}\frac{d^2u}{dt^2}(t_n) + O(\Delta t^3).    
\]
!et
From the FE updated formula introduced above, the approximate solution $\hat{u}_{n+1}$ is given by
!bt
\[
 \hat{u}_{n+1} = u(t_n) + \Delta t \frac{du}{dt}(t_n). 
\]
!et
We see that the error $\| \hat{u}_{n+1}- u_{n+1}\| \sim \Delta x^2$, and since this is the error for a single time step, the
accumulated error after $N\sim 1/Delta t $ steps is proportional to $\Delta t$. The FE method is hence a \emph{first order} method.
As we will see in Chapter ref{ch:runge_kutta}, more accurate methods can be constructed by deriving update formulas to make more
terms in the Taylor expansion cancel. This process is fairly straightforward for low-order methods, e.g., of second or third order, 
but it quickly gets complicated for high order solvers, see, for instance (NBNB ref hairer wanner) for details. 

Knowing the theoretical accuracy of an ODE solver is important for a number of reasons, and one of them is that it
provides a method for verifying our implentation of the solver. If we can solve a given problem and demonstrate that
the error behaves as predicted by the theory, it is a strong indication that the solver is implemented correctly. 
We can illustrate this procedure using the simple initial value problem introduced earlier;
!bt
\[
    \frac{du}{dt} = u, u(0) = 1 .
\]
!et
As stated above, this problem has the analytical solution $u=e^t$, and we can use this to compute the error in our numerical solution.
But how should we define the error in the numerical solution? There is no unique answer to this question. For practical applications,
the so-called root-mean-square (RMS) or relative-root-mean-square (RRMS) are commonly used error measures, defined by
!bt
\begin{align*}
error_{RMS} &=  \sqrt{\frac{1}{n}\sum_{i=0}^N(\hat{u}_i-u(t_i)^2}, 
error_{RRMS} &= \sqrt{\frac{1}{n}\sum_{i=0}^N(\frac{\hat{u}_i-u(t_i)^2}{u(t_i)^2}}, 
\end{align*}
!et
respectively. (NBNB check formulas) In more mathematically oriented texts, the errors are defined in terms of \emph{norms}, with
the discrete $l_1,l_2,$ and $l_\infty$ norms being the most relevant. These are defined as follows;
!bt
\begin{align*}
 e_{l_1} &= sum_{i=0}^N(\|\hat{u}_i-u(t_i)\|),
 e_{l_2} &= \sum_{i=0}^N(\hat{u}_i-u(t_i)^2),
 e_{l_\infty} &= \max_{i=0}^N(\hat{u}_i-u(t_i)).  
\end{align*}
!et
(NBNB check formulas) While the choice of error norm may be important for certain cases, for practical
applications it is usually not very important, and all the different error measures can be expected to behave
as predicted by the theory. For simplicity we will apply an even simpler error measure for our example, where
we simply compute the error at the final time $T$, given by $e = \|\hat{u}_N-u(t_N)\|$. Using the `ForwardEuler`
class introduced above, the complete code for checking the convergence may look as follows:
!bc pycod
from forward_euler_class_v2 import *
import numpy as np

def rhs(u,t):
    return u

def exact(t):
    return np.exp(t)

solver = ForwardEuler(rhs)
solver.set_initial_condition(1.0)

T = 3.0
N = 30

print(f'Time step (dt)   Error (e)      e/dt')
for _ in range(10):
    time = np.linspace(0,T,N+1)
    dt = T/N
    u,t = solver.solve(time)
    e = abs(u[-1]-exact(time[-1]))
    print(f'{dt:<14.7f}   {e:<12.7f}   {e/dt:5.4f}')
    N = N*2
!ec 
This program will produce the following output when it is run in the terminal:
!bc
Time step (dt)   Error (e)      e/dt
0.1000000        2.6361347      26.3613
0.0500000        1.4063510      28.1270
0.0250000        0.7273871      29.0955
0.0125000        0.3700434      29.6035
0.0062500        0.1866483      29.8637
0.0031250        0.0937359      29.9955
0.0015625        0.0469715      30.0618
0.0007813        0.0235117      30.0950
0.0003906        0.0117624      30.1116
0.0001953        0.0058828      30.1200
!ec
In the rightmost column we see that the error divided by the time step is approximately constant, which
supports the theoretical result that the error is proportional to $\Delta t$. In subsequent chapters we will
perform similar calculations for higher order methods, to confirm that the error is proportional to $\Delta t^r$,
where $r$ is the theoretical order of convergence for the method. 

In order to compute the error in our numerical solution we obviously need to know the true solution to our 
initial value problem. This part was easy for the simple example above, since we knew the analytical solution
to the equation, but this solution is only available for very simple ODE problems. It is, however, often 
interesting to estimate the error and the order of convergence for more complex problems, and for this task we 
need to take a different approach. Several alternatives exist, including for instance the *method of manufactured
solutions*, where one simply choses a solution function $u(t)$ and computes its derivative analytically to 
determine the right hand side of the ODE (NBNB ref). An even simpler approach, which usually works well, is to 
compute a very accurate numerical solution using a high-order solver and small time steps, and use this
solution as the reference for computing the error. For accurate error estimates it is of course essential
that the reference solution is considerably more accurate than the numerical solution we want to evaluate. 
The reference solution therefore typically requires very small time steps and can take some time to 
compute, but in most cases the computation time will not be a problem. We will revisit this method for 
some example problems in Chapter (NBNB ref example chapter).












