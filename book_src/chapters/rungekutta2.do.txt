# #if FORMAT == 'ipynb'
========= Stable solvers for stiff ODEs =========
# #endif

* Simple stability analysis
* Examples of stiff systems
* Implicit Runge-Kutta methods
* The BDF method

In the previous chapter we introduced Explicit Runge-Kutta (ERK) methods and observed
how they could conveniently be implemented as a hierarchy of ODE solvers. For most
ODE systems, replacing the simple forward Euler method with a higher-order ERK method
will significantly reduce the number of time steps needed to reach a 
specified accuracy. In most cases it will also lead to a reduced overall computation time,
since the additional cost for each time step is more than outweighed by the reduced number 
of steps. However, for a certain class of ODEs we may observe that all the ERK methods require
very small time steps, and any attempt to increase the time step will lead to spurious
oscillations and possible divergence of the solution. These ODE systems are usually
referred to as *stiff*, and none of the explicit methods introduced in the previous
chapters do a very good job at solving them. We shapp see that implicit solvers such as
Implicit Runge-Kutta (IRK) methods are far better suited for stiff problems, and may give
substantial reduction of the computation time for challenging problems.

======= Stiff ODE system and stability =======
One very famous example of a stiff ODE system is the Van der Pol equation, which can be 
written as an initial value problem on the form
!bt
\begin{align}
\frac{dy_1}{dt} = y_2, y_1(0) = 1, label{vdp1}\\
\frac{dy_2}{dt} = \mu(1-y_1^2)y_2 - y_1, y_2(0) = 0. label{vdp2}
\end{align}
!et
The parameter $\mu$ is a constant which determines the properties of the system, including 
its "stiffness". For $\mu=0$ the problem is a simple oscillator with analytical solution
$y_1 = \cos(t),y_2=\sin(t)$, while for non-zero values of $\mu$ the solution shows
far more complex behavior. The following code implements this system and solves it with the
`ForwardEuler` class.
!bc pycod
from ODESolver import *
import numpy as np
import matplotlib.pyplot as plt

class VanderPol:
    def __init__(self,mu):
        self.mu = mu

    def __call__(self,u,t):
        du1 = u[1]
        du2 = self.mu*(1-u[0]**2)*u[1]-u[0]
        return du1,du2


model = VanderPol(mu=5)

solver = ForwardEuler(model)
solver.set_initial_condition([1,0])

time = np.linspace(0,20,2001)
u,t  = solver.solve(time)

plt.plot(t,u)
plt.show()
!ec
Figure ref{fig:vanderpol} shows the solutions for $\mu=0,1$ and 5. Setting $\mu$ even higher, for instance
$\mu=50$, leads to a divergent (unstable) solution, and replacing the Forward Euler method with a more
accurate ERK method does not help much. It helps to reduce the time step dramatically, but the computation
time may be substantial. The time step is dictated by stability requirements rather than our desired
accuracy, and there may be significant gains from choosing a more stable solver. 

Why does the solution of the Van der Pol model fail so badly for large $\mu$? And, more generally,
what are the properties of an ODE system that makes it stiff? If we, for a moment, consider the
simpler case of a linear ODE, stiffness is related to the eigenvalues of the problem. 
Consider, for instance, a simple ODE known as the Dahlquist test equation;
!bt
\begin{align}
u' &= \lambda u, u(0) &= 1,  label{linear} 
\end{align}
!et
where $\lambda$ may be a complex number. As defined in
(ref {AscherPetzold}), this equation is stiff for an interval
$[0,b]$ if the real part of $\lambda$ satisfies
\[
b\Re(\lambda) \ll -1 .
\]
Note that we assume
\[
\Re(\lambda) \leq 0 ,
\]
since this is required for the equation to be stable. For a general non-linear
problem, such as the Van der Pol in (ref{vdp1})-(ref{vdp2}), the
system's stiffness is characterized by the eigenvalues $\lambda_i$ of the local
Jacobian matrix $J$ of the right hand side function $f$. The Jacobian is defined by 
\[
J_{ij} = \frac{\partial f_i (t,y)}{\partial y_j} ,
\]
and the problem is stiff for an interval $[0,b]$ if
\[
b\min_{i}\Re(\lambda_i) \ll -1.
\]
In the ODE literature one will also find more pragmatic definitions
of stiffness, for instance that an equation is stiff if the 
time step needed to maintain stability of an explicit
method is much smaller than the time step dictated by the accuracy
requirements (NBNB ref {Curtiss52,AscherPetzold}). These definitions indicate that
the stiffness of a problem is not only a function of the ODE itself,
but also of the interval of integration and of the chosen accuracy requirement. A
detailed discussion of stiff ODE systems can be found in, for instance,
(NBNB ref {AscherPetzold,ODEII}).

Eq. (ref{linear}) is the foundation for *linear stability analysis*, which is a very
useful technique for analyzing and understanding the stability of ODE solvers. 
The solution to the equation is $u(t) = \exp(\lambda t)$, which, as noted above,
is unstable for $\lambda$ with a positive real part. We are therefore primarily 
interested in the case $\Re(\lambda) < 0$, for which the solution is stable but 
our choice of solver may introduce *numerical instabilities*. A single step of the
Forward Euler method applied to (ref{linear}) gives
!bt
\[
    u_n = u_n +\Delta t \lambda u_0 = u_n (1+\Delta t \lambda) ,
\]
!et
and for the first step, since we have $u(0) = 1$, we have
!bt
\begin{align}
    u_1 &= u(\Delta t) =  1+\Delta t \lambda . label{linear_euler1}
\end{align}
!et
Since the analytical solution decays exponentially for $\Re(\lambda) < 0$, it is natural to 
require the same behavior of the numerical solution, and this gives the requirement
that $\| 1+\Delta t \lambda \| \leq 1$. If $\lambda$ is real and negative, the time
step must be chosen to satisfy $\Delta t \leq -2/\lambda$ to ensure stability. 
Figure (NBNB ref new fig) shows the FE solution to (ref{linear}) with $\lambda = -5$. The 
left panel shows the solution for $\Delta t = 0.21$, while the right panel is for 
$\Delta t = 0.19$. (NBNB check this)

We have observed that the right hand side of (ref{linear_euler1}) contains critical information
about the stability of the FE method. In fact, this expression is often called the 
*stability function* of the method, and written on the general form
!bt
\[
    R(z) = 1+z .
\]
!et
The FE method is stable for all values $\lambda \Delta t$ which give $\|R(\lambda \Delta t)\| <1$,
and this region of $\lambda \Delta t$ values in the complex plane is referred to as the method's
*region of absolute stability*, or simply its *stability region*. 
The stability region for the FE method is shown in the left panel of Figure (NBNB ref ny fig). Obviously, for any $\lambda \ll 0$, 
this stability criterion is quite restrictive for the choice of $\Delta t$. 

We can easily extend the linear stability analysis to the other explicit RK methods introduced in Chapter ref{ch:runge_kutta}.
For instance, applying a single step of the explicit midpoint method given by (ref{midpoint0})-(ref{midpoint2}) to 
(ref{linear}) gives 
!bt
\[
u(\Delta t)  = 1 + \lambda\Delta t + \frac{(\Delta t\lambda)^2}{2} ,
\]
!et
and we may identify the stability function for this method as 
!bt
\[
R(z) = 1 + z + \frac{z^2}{2}.
\]
!et
The corresponding stability region is shown in the middle panel of Figure (NBNB ref ny fig). For the 
fourth order RK method defined in (ref{rk4_0})-(ref{rk4_5}), the same steps reveal that the stability function is
!bt 
\[
    R(z) = 1 + z + \frac{z^2}{2} + \frac{z^3}{6} + \frac{z^4}{24}, 
\] 
!et
(NB sjekk denne!!!) and the stability region is shown in the right panel of Figure (NBNB ref ny fig). We observe
that the stability of the two higher-order RK methods are larger than that of the FE method, but not much. In fact, 
if we consider the computational cost of each time step for these methods, the FE method is superior 
superior for problems where the time step is governed by stability. It can be shown that the stability function
for an $s$-stage explicit RK method is always a polynomial of degree $\leq s$. To obtain a singificant improvement of this situation, 
we typically need to replace the explicit methods considered so far with implicit RK methods.

======= Implicit methods for stability =======
Since (ref{linear0}) is stable for all values of $\lambda$ with a negative real part, it is natural to look
for numerical methods with the same property. This means that the stability domain for the method 
covers the entire left half of the complex plane, or that its stability function $|R(z)|< 1$ whenever $\Re(z) <0$. 
(NBNB sjekk < vs <=) This property is called *A-stability*. As noted above, the stability 
function of an explicit RK method is always a polynomial, and no polynomial satisfies $|R(z)|< 1$ for all $z<0$. 
Therefore, there are no A-stable explcit RK methods. 
An even  stronger stability requirement can be motivated 
by the fact that for $\lambda \ll 0$, the solution decays very rapidly. It is natural to expect the same
behavior of the numerical solution, by requiring $|R(z)|\rightarrow 0$ as $z\rightarrow -\infty$. This property is 
referred to as *stiff decay*, and an A-stable method that also has stiff decay is called an *A-stable* method. 

The simplest implicit RK method is the backward Euler (BE) method, which can be derived in exactly the same way 
as the FE method, by approximating the derivative with a simple finite difference. The only difference
from the FE method is that the right hand side is evaluated at step $n+1$ rather than step $n$. 
For a general ODE, we have
!bt
\[
\frac{u_{n+1}-u_n}{\Delta t} = f(u_{n+1},t_{n+1}),
\]
!et
and if we rearrange the terms we get
!bt
\begin{equation}
u_{n+1} - \Delta t f(u_{n+1},t_{n+1}) = u_n  .  \label{be_nonlin0}
\end{equation}
!et
Although the derivation is very similar to the FE method, there is a 
fundamental difference in that the unknown $u_{n+1}$ occurs as an argument in the right hand 
side function $f(u,t)$. Therefore, for nonlinear $f$, (ref{be_nonlin0}) is a 
nonlinear algebraic equation that must be solved for the unknown $u_{n+1}$, 
instead of the explicit update formula we had for the FE method. 
Eq. (ref{be_nonlin0}) is typically solved using a variant of Newton's method. This requirement 
makes implicit methods more complex to implement than explicit methods, and they tend to 
be require far more computations per time step. Still, as we will demonstrate later, the
superior stability properties still make implicit solvers superior for stiff problems. 

We will study the details of solving (ref{be_nonlin0}) in Section ref{sec:irk_newton} below, but
let us first study the stability of the BE method and other implicit RK solvers using the
linear stability analysis introduced above. Applying the 
BE method to (ref{linear0}) yields 
!bt
\[
u_{n+1} (1-\Delta t\lambda) = u_n,    
\]
!et
and for the first time step, with $u(0) = 1$, we get
!bt
\[
u_1 = \frac{1}{1-\Delta t\lambda} .    
\]
The stability function of the BE method is therefore $R(z) = 1/(1-z)$, and the corresponding stability domain 
shown in the left panel of Figure (NB ref ny fig). The method is stable for all choices of $\lambda \Delta t$
*outside* the circle with radius one and center at (1,0) in the complex plane, confirming that the BE 
is a very stable method. It is A-stable, since the stability domain covers the entire left half of the
complex plane, and it is also L-stable since we have $R(z) \rightarrow 0$ as $\Re(z) \rightarrow -\infty$.

The BE method fits into the general RK framework introduced by (ref{genrk0})-(ref{genrk1}) in Chapter 
ref{ch:runge_kutta}, with a single stage ($s=1$), and $a_{11} = b_1 = c_1 = 1$. The explicit midpoint and
trapezoidal methods mentioned above also have their implicit counterpart,

The Butcher tableau is 
!bt
\[
\begin{array}{c|cc}
1 & 1 \\ \hline
 & 1
\end{array} .
\]
!et
o Implicit midpoint (maybe before the butcher tableau??)
o Radeau
o DIRK/SDIRK?

