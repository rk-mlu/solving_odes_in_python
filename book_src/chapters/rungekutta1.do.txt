# #if FORMAT == 'ipynb'
========= High-order explicit ODE solvers =========
# #endif

As mentioned earlier, the FE method derived in Chapter ref{ch:ode_intro} is not the
most sophisticated ODE solver, although it is sufficiently accurate for
most of the applications we will consider in this book. Many alternative methods
exist, which have better accuracy and stability and are therefore better suited
for solving challenging ODE systems. In this chapter we will focus
on improving the accuracy, which means that we will stick with explicit
methods. Implicit methods, which have better stability properties and are more
suitable for so-called *stiff* ODEs, will be
considered in Chapter ref{ch:stiff}.

In Chapter ref{ch:ode_intro} we demonstrated that the FE method is a
first order accurate method, which means that the error in the numerical
solution is proportional to the time step size $\Delta t$. In this chapter
we will derive solvers of higher order, for which the numerical error
is proportional to a higher power of $\Delta t$.
To illustrate how such higher order ODE solvers can be derived, we return to the
general formulation of the ODE system:
!bt
\[
u' = f(t,u), \quad u(t_0) = u_0 .
\]
!et
We derived the FE method by simply replacing
the derivative with a finite difference approximation, but in the present 
chapter we will apply a slightly different approach to motivate higher order solvers.
Assuming that we know the solution
$u_n$ at time $t_n$, the solution at time $t_{n+1}$ can  be found by
integrating both sides of the equation. We have 
!bt
\[
\int_{t_n}^{t_{n+1}} \frac{du}{dt} dt =  \int_{t_n}^{t_{n+1}}
f(t,u(t)) dt ,
\]
!et
which gives us the exact solution at time $t_{n+1}$ as
!bt
\begin{equation}
u(t_{n+1}) = u(t_n) + \int_{t_n}^{t_{n+1}} f(t,u(t)) dt .
\label{ode_integral0}\end{equation}
!et
It is in general not possible to compute the integral on the right-hand side
analytically, since $f$ is often non-linear and the function $u(t)$ is unknown.
However, we can approximate the integral using a variety of numerical
integration techniques. The simplest approximation is to set
!bt
\[
f(t,u(t)) \approx f(t_n,u_n), \text{ for } t_n < t < t_{n+1},
\]
!et
i.e., approximate the integrand as a constant on the interval $t_n$ to $t_n +\Delta t$.
Inserting this choice in (ref{ode_integral0}) gives the update formula
!bt
     \[ u_{n+1} = u_n + \Delta t\, f(t_n, u_n), \]
!et
which we recognize as the FE method introduced in Chapter ref{ch:ode_intro}.
Approximating the function $f(t_n,u_n)$ as constant on the interval
$t_n < t < t_{n+1}$ is obviously not the most accurate choice, and we shall see
that more accurate approximations of this integrand gives rise to ODE solvers
of higher order.

The classical way to approximate the integral of a general non-linear function is
to approximate the function by a polynomial, and then integrating this polynomial
analytically. This approach forms the basis for standard quadrature rules for numerical
integration, and has also been used to derive accurate ODE solvers. Two main ideas 
have been explored for constructing the polynomial approximation of $f(t,u)$, and have
led to two important classes of ODE solvers. The first approach is to approximate $f(t,u)$ 
by a polynomial which interpolates $f$ in previous time points, i.e., 
$f(t_{n-1}, u_{n-1}), f(t_{n-2}, u_{n-2}),\ldots $. This method gives rise to the so-called
*multistep* methods, which are widely used for solving ODEs. We will not consider multistep
methods in this book, but the interested reader is referred to, for instance, cite{ODEI,AscherPetzold}.
The second approach is to compute a number of intermediate approximations of $f(t,u)$ on the
interval $t_n < t < t_{n+1}$, and use these values to define the polynomial approximation of $f$.
This idea is similar to how classical quadrature rules for numerical integration are derived, 
and gives rise to a class of ODE solvers known as Runge-Kutta methods. RK methods come in
many forms, with very different accuracy and stability properties, and will be the main topic of 
chapters ref{ch:runge_kutta}-ref{ch:adaptive}.


======= Explicit Runge-Kutta methods =======
idx{Runge-Kutta method !explicit}
As noted above, an intuitive way to improve the accuracy of the approximate integral in
(ref{ode_integral0}) is
to compute a number of intermediate approximations of $f(t_*, u_*)$ for
 $t_n \leq t_* \leq t_{n+1}$, and compute the integral as a weighted sum of
these values. This approach builds on standard techniques of numerical
integration, and gives rise to a very popular class of ODE solvers
known as *Runge-Kutta (RK) methods*. The simplest example of an RK method
is in fact the Forward Euler (FE) method considered earlier, which is an example of a one-stage, first-order,
explicit RK method. An alternative formulation of the FE method is
!bt
\begin{align*}
k_1 &= f(t_n, u_n), \\
u_{n+1} &= u_n + \Delta t k_1 .
\end{align*}
!et
It can easily be verified that this is the same formula as introduced
above, and there is no real benefit from writing the formula in two lines
rather than one. However, this second formulation is more in line with
how RK methods are usually written, and it makes it easy to see the
relation between the FE method and more advanced solvers. The intermediate
value $k_1$ is often referred to as a *stage derivative* in the
ODE literature.
idx{stage derivative} idx{stages (of Runge-Kutta methods)}

idx{midpoint method !explicit}
We can easily improve the accuracy of the FE method to
second order, i.e., error proportional to $\Delta t^2$, by introducing more
accurate approximations of the integral in (ref{ode_integral0}). One option
is to keep the assumption that $f(t,u(t))$ is constant 
over $t_n \leq t_* \leq t_{n+1}$, but to approximate it at the middle of the
interval rather than the left end. This approach requires one additional stage:
!bt
\begin{align}
k_1 & = f(t_n, u_n), \label{midpoint0}\\
k_2 & = f(t_n+\frac{\Delta t}{2}, u_n+\frac{\Delta t}{2} k_1), \label{midpoint1}\\
u_{n+1} &= u_n + \Delta t\, k_2 . \label{midpoint2}
\end{align}
!et
This method is known as the explicit midpoint method or the modified Euler method.
The first step is identical to that of the FE method, but instead
of using the stage derivate $k_1$ to advance the solution to the next step,
we use it to compute an intermediate midpoint solution 
!bt
\[
    u_{n+1/2} = u_n+\frac{\Delta t}{2} k_1 .
\]
!et
This solution is then used to compute the corresponding stage derivative $k_2$, 
which becomes an approximation to the derivative of $u$ at time $t_n+\Delta t/2$. 
Finally, we use this midpoint derivative to advance the solution to $t_{n+1}$. 

idx{Heun's method}idx{trapezoidal method !explicit}
An alternative second order method is Heun's method,
also referred to as the explicit trapezoidal method, which can be derived
by approximating the integral in (ref{ode_integral0}) by a trapezoidal rule:
!bt
\begin{align}
k_1 & = f(t_n, u_n), \\
k_2 & = f(t_n+\Delta t, u_n+\Delta t k_1), \\
u_{n+1} &= u_n + \frac{\Delta t}{2} (k_1 + k_2) .
\end{align}
!et
This method also computes two stage derivatives $k_1$ and $k_2$, but from the
formula for $k_2$ we see that it approximates the derivative at $t_{n+1}$ rather
than at the midpoint $t_{n}+\Delta t/2$. The solution is then advanced from
$t_n$ to $t_{n+1}$ using the mean value of $k_1$ and $k_2$.

All RK methods follow the same recipe as the two second-order methods considered
above; we compute one or more intermediate values (i.e., stage derivatives), and then advance the
solution using a combination of these stage derivatives. The accuracy of the method can
be improved by adding more stages. A general RK method with $s$ stages
can be written as
!bt
\begin{align}
k_i &= f(t_n+c_i\Delta t, u_n+\Delta t \sum_{j=1}^s a_{ij}k_j),  \mbox{\  for } i = 1,\ldots ,s \label{genrk0}\\
u_{n+1} &= u_n + \Delta t \sum_{i=1}^s b_i k_i .\label{genrk1}
\end{align}
!et
Here $c_i, b_i, a_{ij}$, for $i,j, = 1,\ldots ,s$ are
method-specific coefficients. All RK methods can be written
in this form, and a method is uniquely determined by the
number of stages $s$ and the values of the coefficients. 
 idx{Runge-Kutta method !general formula}

As mentioned above, there exists a wide variety of RK methods, 
where the coefficients are typically chosen to optimize the accuracy for
a given number of stages. We will not dive into the details of how the
methods are constructed here, but some of the principles can be quite useful to know about.
For instance, it can be shown that the $b_i$ coefficients must be chosen to satisfy
$\sum_{i=1}^s b_i = 1$ in order to have a convergent method. This condition follows quite
naturally from our motivation for RK methods as numerical integrators 
applied to (ref{ode_integral0}). When approximating the integral as a weighted sum, the sum of the weights must
obviously be one. Another common constraint on the coefficients is to 
set $c_i = \sum_{j=1}^s a_{ij}$. While this constraint
is not strictly needed, it may simplify the derivation of the methods and 
follows naturally from our interpretation of the stage derivative $k_i$ as 
approximations of the right-hand side $f(t,u)$ at time $t_n+c_i\Delta t$. When implementing a 
new solver it is very easy to introduce errors in the coefficient values, and it may be useful to 
include tests to verify that the most fundamental conditions on the coefficients are satisfied.
It is possible to derive general *order conditions* that the coefficients must satisfy for a method
to be of a given order, see, for instance, cite{AscherPetzold,ODEI} for details. 
In this chapter we only consider explicit Runge-Kutta (ERK) methods, which means that $a_{ij} = 0$
for $j\geq i$. It can be shown that the order $p$ of an ERK method with $s$ 
stages satisfies $p\leq s$, and for $p \geq 5$ the bound is $p\leq s-1$. However, it is not known
whether the last bound is sharp, and it may be even stricter for methods of very high order. For 
instance, all known methods with $p=8$ have at least eleven stages, and it is not known whether eight-order methods
with nine or ten stages exist. 

idx{Butcher tableau}
In the ODE literature the method coefficients are often specified in the
form of a *Butcher tableau*, which offers a compact definition of any RK method.
The Butcher tableau is simply a specification of all the method coefficients, and
for a general RK method it is written as
!bt
\[
\begin{array}{c|ccc}
c_i & a_{11} & \cdots & a_{1s}\\
\vdots & \vdots & & \vdots \\
c_s & a_{s1} & \cdots & a_{ss} \\ \hline
 & b_1 & \cdots & b_s
\end{array} 
\]
!et
The Butcher tableaus of the three methods considered above; FE, explicit midpoint, and Heun's method, are
!bt
\[
\begin{array}{c|cc}
0 & 0 \\ \hline
 & 1
\end{array} \hspace{1mm}, \hspace{1cm}
\begin{array}{c|cc}
0 & 0& 0 \\
1/2 & 1/2 & 0 \\ \hline
 & 0 & 1
\end{array} \hspace{1mm},\hspace{1cm}
\begin{array}{c|cc}
0 & 0& 0 \\
1 & 1 & 0 \\ \hline
 & 1/2 & 1/2
\end{array} \hspace{1mm},
\]
!et
respectively. To grasp the concept of Butcher tableaus, it is a good exercise to
insert the coefficients from these three tableaus into (ref{genrk0})-(ref{genrk1})
and verify that you arrive at the correct formulae for the three methods.
As an example of a higher order method, we may consider the "original" Runge-Kutta 
method, which is a fourth-order, four-stage method defined by 
!bt
\[
\begin{array}{c|cccc}
0 & 0& 0 & 0 & 0\\
1/2 & 1/2 & 0 & 0 & 0\\
1/2 & 0 & 1/2 & 0 & 0 \\
1  & 0 & 0 & 1 & 0 \\ \hline
 & 1/6 & 1/3 & 1/3 & 1/6
\end{array} ,
\]
!et
which gives the formulas
!bt
\begin{align}
k_1 &= f(t_n, u_n) , \label{rk4_0}\\
k_2 &= f(t_n + \frac{\Delta t}{2}, u_n + \frac{\Delta t}{2}k_1),\\
k_3 &= f(t_n + \frac{\Delta t}{2}, u_n + \frac{\Delta t}{2}k_2),\\
k_4 &= f(t_n + \Delta t, u_n + \Delta t k_3), \\
u_{n+1} &= u_n + \frac{\Delta t}{6} \left( k_1 + 2k_2 + 2k_3 + k_4\right) .\label{rk4_5}
\end{align}
!et
As mentioned above, all the methods considered in this chapter are 
explicit methods, which means that $a_{ij} = 0$ for $j \geq i$. As may be observed from 
(ref{rk4_0})-(ref{rk4_5}), or from a more careful inspection of the general formula (ref{genrk0}),
this means that the expression for computing each stage derivative $k_i$ only includes
previously computed stage derivatives. Therefore, all $k_i$ can be computed sequentially
using explicit formulae. For implicit RK methods, on the other hand,
we have $a_{ij} \neq 0$ for some $j\geq i$. We see from
(ref{genrk0}) that the formula for computing $k_i$ will then include $k_i$
on the right-hand side, as part of the argument to the function $f$. We
therefore need to solve equations to compute the stage derivatives, and since
$f$ is typically non-linear we need to solve these equations with an iterative solver
such as Newton's method. These steps make implicit
RK methods more complex to implement and more computationally expensive per time step,
but they are also more stable than explicit methods and perform much better
for certain classes of ODEs. We will consider implicit RK methods in
Chapter ref{ch:stiff}.

======= A class hierarchy of Runge-Kutta methods =======
label{sec:rk_hierarchy}
idx{class ! hierarchy} idx{class !superclass/base class} 
We now want to implement RK methods as classes,
similar to the FE classes introduced above. When inspecting the `ForwardEuler`
class, we quickly observe that most of the code is common to all ODE solvers, and
not specific to the FE method. For instance, we
always need to create an array for holding the solution,
and the general solution method using a for-loop is always the same. In fact, the only
difference between the different methods is how the solution is advanced from one
step to the next. Recalling the ideas of Object-Oriented Programming, it becomes
obvious that a class hierarchy is convenient for implementing such a collection
of ODE solvers. In this way we can collect all common code in a superclass (base class), and rely on
inheritance to avoid code duplication. The superclass can handle most of the
more administrative steps of the ODE solver, such as
  * Storing the solution $u_n$ and the time points $t_n$, $k=0,1,2,\ldots,n$
  * Storing the right-hand side function $f(t,u)$
  * Storing and applying the initial condition
  * Running the loop over all time steps

idx{`ODESolver` class}
We can introduce a superclass `ODESolver` to handle these parts, and implement
the method-specific details in subclasses. It should now become quite obvious why
we chose to isolate the code to perform a single step in the `advance` method,
since this will then be the only method we need to implement in the subclasses.
The implementation of the superclass can be quite similar to the `ForwardEuler`
class introduced earlier:
!bc pycod
import numpy as np

class ODESolver:
    def __init__(self, f):
        # Wrap user's f in a new function that always
        # converts list/tuple to array (or let array be array)
        self.model = f
        self.f = lambda t, u: np.asarray(f(t, u), float)

    def set_initial_condition(self, u0):
        if np.isscalar(u0):              # scalar ODE
            self.neq = 1                 # no of equations
            u0 = float(u0)
        else:                            # system of ODEs
            u0 = np.asarray(u0)
            self.neq = u0.size           # no of equations
        self.u0 = u0

    def solve(self, t_span, N):
        """Compute solution for t_span[0] <= t <= t_span[1],
        using N steps."""
        t0, T = t_span
        self.dt = (T - t0) / N
        self.t = np.zeros(N + 1)  # N steps ~ N+1 time points
        if self.neq == 1:
            self.u = np.zeros(N + 1)
        else:
            self.u = np.zeros((N + 1, self.neq))

        msg = "Please set initial condition before calling solve"
        assert hasattr(self, "u0"), msg

        self.t[0] = t0
        self.u[0] = self.u0

        for n in range(N):
            self.n = n
            self.t[n + 1] = self.t[n] + self.dt
            self.u[n + 1] = self.advance()
        return self.t, self.u

        def advance(self):
            raise NotImplementedError(
                "Advance method is not implemented in the base class")
!ec
Notice that the `ODESolver` is meant to be a pure superclass, and the implementation of the
`advance` method is left for subclasses. In order to make this abstract nature of the
class explicit, we have implemented an `advance` method that will simply raise 
a `NotImplementedError` when it is called. If we try to make an instance of `ODESolver` and
use it as a stand-alone solver, we will get an error in the 
line `self.u[n + 1] = self.advance()`. If we had left 
out the definition of `advance` completely we would get an error from the same line, but
it would be a less informative `AttributeError`. Raising the `NotImplementedError` makes
it clear to anyone reading or using the code that this behavior is intentional, and that
the functionality is to be implemented in subclasses. 
It should be noted that there are alternative ways in Python to make explicit the abstract nature of the `ODESolver` class, 
for instance using the module `abc`, for "Abstract Base Class". However, while this
solution may be considered more modern, we have decided to not use it here, in the interest of
keeping the code simple and compact. 
idx{class !abstract base class}

With the superclass at hand, the implementation of a `ForwardEuler` subclass
becomes very simple:
!bc pycod
class ForwardEuler(ODESolver):
    def advance(self):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = self.dt
        return u[n] + dt * f(t[n], u[n])
!ec
Similarly, the explicit midpoint method and the fourth-order RK method can be subclasses, each
implementing a single method:
!bc pycod
class ExplicitMidpoint(ODESolver):
    def advance(self):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = self.dt
        dt2 = dt / 2.0
        k1 = f(t[n], u[n])
        k2 = f(t[n] + dt2, u[n] + dt2 * k1)
        return u[n] + dt * k2

class RungeKutta4(ODESolver):
    def advance(self):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = self.dt
        dt2 = dt / 2.0
        k1 = f(t[n], u[n],)
        k2 = f(t[n] + dt2, u[n] + dt2 * k1, )
        k3 = f(t[n] + dt2, u[n] + dt2 * k2, )
        k4 = f(t[n] + dt, u[n] + dt * k3, )
        return u[n] + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)
!ec

The use of these classes is nearly identical to the FE class introduced
in Section ref{sec:ode_sys}. Considering
the same simple ODE used above; $u'= u$, $u(0)=1$, $t\in [0,3]$, $\Delta t=0.5$,
the code looks like:
!bc pycod
import numpy as np
import matplotlib.pyplot as plt
from ODESolver import ForwardEuler, ExplicitMidpoint, RungeKutta4

def f(t, u):
    return u

t_span = (0, 3)
N = 6

fe = ForwardEuler(f)
fe.set_initial_condition(u0=1)
t1, u1 = fe.solve(t_span, N)
plt.plot(t1, u1, label='Forward Euler')

em = ExplicitMidpoint(f)
em.set_initial_condition(u0=1)
t2, u2 = em.solve(t_span, N)
plt.plot(t2, u2, label='Explicit Midpoint')

rk4 = RungeKutta4(f)
rk4.set_initial_condition(u0=1)
t3, u3 = rk4.solve(t_span, N)
plt.plot(t3, u3, label='Runge-Kutta 4')

# plot the exact solution in the same plot
time_exact = np.linspace(0, 3, 301)
plt.plot(time_exact, np.exp(time_exact), label='Exact')
plt.title('RK solvers for exponential growth, $\\Delta t = 0.5$')
plt.xlabel('$t$')
plt.ylabel('$u(t)$')
plt.legend()
plt.show()
!ec
This code will solve the same simple equation using three different methods, and plot the solutions
in the same window, as shown in Figure ref{fig:simplecompare}. We set `N = 6`, which corresponds to 
a very large time step ($\Delta t = 0.5$), to highlight the difference in accuracy between the methods.

FIGURE: [../chapters/figs_ch2/ch2_simplecompare.pdf, width=600 frac=1] Numerical solutions of the exponential growth problem, computed with `ForwardEuler`, `ImplicitMidpoint` and `RungeKutta4`. All the solvers use $\Delta t = 0.5$, to highlight the difference in accuracy. label{fig:simplecompare}


======= Testing the solvers =======
In Chapter ref{ch:ode_intro} we showed how to compute the error in the numerical solution, and in particular how we
could verify that the error behaved as predicted by the theoretical convergence of the applied solvers. 
Such tests are extremely valuable for verifying that we have implemented the ODE solvers correctly, and 
can easily be extended to the higher order solvers. As an example, the following code defines a dictionary
containing three different solver classes and their theoretical order, and solves the simple exponential 
ODE for all three solvers.  
!bc pycod
from ODESolver import *
import numpy as np


def rhs(t, u):
    return u


def exact(t):
    return np.exp(t)

solver_classes = [(ForwardEuler,1), (Heun,2), 
                  (ExplicitMidpoint,2), (RungeKutta4,4)]

for solver_class, order in solver_classes:
    solver = solver_class(rhs)
    solver.set_initial_condition(1.0)

    T = 3.0
    t_span = (0, T)
    N = 30
    print(f'{solver_class.__name__}, order = {order}')
    print(f'Time step (dt)   Error (e)     e/dt**{order}')
    for _ in range(10):
        t, u = solver.solve(t_span, N)
        dt = T / N
        e = abs(u[-1] - exact(T))
        if e < 1e-13:  # break if error is close to machine precision
            break
        print(f'{dt:<14.7f}   {e:<12.7f}  {e/dt**order:5.4f}')
        N = N * 2
!ec 
The code is nearly identical to the FE convergence test in Section ref{sec:error_taylor},
except that we loop over a list of tuples that contain the four method classes and their corresponding
order. The output is also nearly identical to the previous version, but repeated 
for all four solvers, and we use the built-in class attribute `__name__` to extract and
print the name of each solver. Three columns are written to the screen, containing, respectively, the
time step $\Delta t$, the error $e$ at time $t=3.0$, and finally $e/\Delta t^p$, where $p$ is the
order of the method. For the two first methods the output is exactly as expected, and it is
therefore not shown here. The numbers in the rightmost column are approximately constant, 
confirming that the error is in fact proportional to $\Delta t^p$.  
However, the last part of the output, for the forth order RK method, looks like this:
!bc dat
RungeKutta4 order = 4
Time step (dt)   Error (e)     e/dt**4
0.1000000        0.0000462     0.4620
0.0500000        0.0000030     0.4817
0.0250000        0.0000002     0.4918
0.0125000        0.0000000     0.4969
0.0062500        0.0000000     0.4995
0.0031250        0.0000000     0.5006
0.0015625        0.0000000     0.5025
0.0007813        0.0000000     0.5436
0.0003906        0.0000000     5.1880
0.0001953        0.0000000     102.5391
!ec
We see that the $e/\Delta t^p$ numbers are close to constant for a while, in accordance with the
convergence order of the method, but then increase for the smallest values of $\Delta t$. 
This behavior is not uncommon to observe in convergence tests like this, in particular
for high-order methods, and it is caused by the finite accuracy of number representation on a computer. 
As the numerical errors become smaller and approach the machine precision ($\approx 10^{-16}$), 
roundoff error start to dominate the overall error and convergence is lost. 

There are many alternative ways to check the implementation of ODE solvers. One option is to 
consider an even simpler ODE, where the right hand side is a constant, i.e., 
$u'(t) = f(t,u) = C$. The solution to this simple ODE is of course $u(t) = Ct + u_0$, where $u_0$ is
the initial condition. All the numerical methods considered in this book will capture
this solution to machine precision, and we can write a general test function which
takes advantage of this:
!bc pycod
def test_exact_numerical_solution():
    solver_classes = [ForwardEuler, Heun, 
                      ExplicitMidpoint, RungeKutta4]
    a = 0.2
    b = 3

    def f(t, u):
        return a  

    def u_exact(t):
        """Exact u(t) corresponding to f above."""
        return a * t + b

    u0 = u_exact(0)
    T = 8
    N = 10
    tol = 1E-14
    t_span = (0, T)
    for solver_class in solver_classes:
        solver = solver_class(f)
        solver.set_initial_condition(u0)
        t, u = solver.solve(t_span, N)
        u_e = u_exact(t)
        max_error = abs((u_e - u)).max()
        msg = f'{solver_class.__name__} failed, error={max_error}'
        assert max_error < tol, msg
!ec
Similar to the convergence check illustrated below, this code will loop through 
all the solver classes, solve the simple ODE, and check that the resulting error
is within the tolerance. 

Both of the methods shown here to verify the implementation of our solvers have 
some limitations. The most important one is that they both solve very simple ODEs, 
and it is entirely possible to introduce errors in the code that will only present themselves 
for more complex problems. However, they have the advantage of being simple and
completely general, and can easily be applied to any newly implemented ODE solver class.
Many common implementation errors, for instance getting a single parameter wrong in an
RK method, will often show up even for this simple problems. They can therefore
provide a good initial indication that the implementation is correct, which 
can be followed by more extensive tests if needed. 